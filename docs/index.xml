<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving
and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">

<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">

<front>


<article-meta>


<title-group>
<article-title>Investigating metadata discrepancies between OpenAlex and
the Web of Science: the case of Library and Information
Sciences</article-title>
</title-group>

<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">0000-0003-1021-059X</contrib-id>
<name>
<surname>Mongeon</surname>
<given-names>Philippe</given-names>
</name>
<string-name>Philippe Mongeon</string-name>

<email>PMongeon@dal.ca</email>
<xref ref-type="corresp" rid="cor-1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1021-059X</contrib-id>
<name>
<surname>Hare</surname>
<given-names>Madelaine</given-names>
</name>
<string-name>Madelaine Hare</string-name>

</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1021-059X</contrib-id>
<name>
<surname>Krause</surname>
<given-names>Geoff</given-names>
</name>
<string-name>Geoff Krause</string-name>

</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1021-059X</contrib-id>
<name>
<surname>Marjoram</surname>
<given-names>Rebecca</given-names>
</name>
<string-name>Rebecca Marjoram</string-name>

</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1021-059X</contrib-id>
<name>
<surname>Riddle</surname>
<given-names>Poppy</given-names>
</name>
<string-name>Poppy Riddle</string-name>

</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1021-059X</contrib-id>
<name>
<surname>Toupin</surname>
<given-names>Remi</given-names>
</name>
<string-name>Remi Toupin</string-name>

</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1021-059X</contrib-id>
<name>
<surname>Wilson</surname>
<given-names>Summer</given-names>
</name>
<string-name>Summer Wilson</string-name>

</contrib>
</contrib-group>
<author-notes>
<corresp id="cor-1">PMongeon@dal.ca</corresp>
</author-notes>









<history></history>


<abstract>
Bibliometrics, whether used for research or research evaluation, relies
on large multidisciplinary databases of research outputs and citation
indices. The Web of Science (WoS) was the main supporting infrastructure
of the field for more than 30 years until several new competitors
emerged. OpenAlex, a bibliographic database launched in 2022, has
distinguished itself for its openness and extensive coverage. While
OpenAlex may reduce or eliminate barriers to accessing bibliometric
data, one of the concerns that hinders its broader adoption for research
and research evaluation is the quality of its metadata. This study aims
to assess metadata quality in OpenAlex and WoS, focusing on document
type, publication year, language, and number of authors. By addressing
discrepancies and misattributions in metadata, this research seeks to
enhance awareness of data quality issues that could impact bibliometric
research and evaluation outcomes.
</abstract>
<kwd-group kwd-group-type="author">
<kwd>bibliometrics</kwd>
<kwd>research evaluation</kwd>
<kwd>OpenAlex</kwd>
<kwd>Web of Science</kwd>
<kwd>open data</kwd>
<kwd>metadata</kwd>
</kwd-group>




</article-meta>

</front>

<body>
<sec id="introduction">
  <title>Introduction</title>
  <p>Bibliometrics has been used in research and data-driven research
  evaluation for approximately half a century
  (<xref alt="Narin, 1976" rid="ref-narin1976evaluative" ref-type="bibr">Narin,
  1976</xref>), originally supported by the development of the Web of
  Science (WoS), which was conceptualized in 1955 and launched in 1964
  (<xref alt="Clarivate, n.d." rid="ref-clarivate_history" ref-type="bibr">Clarivate,
  n.d.</xref>). It took more than 30 years for other players to emerge:
  Elsevier’s Scopus was founded in 1996, Crossref in 1999, Google
  Scholar in 2004, Microsoft Academic (now integrated into OpenAlex) and
  Dimensions in 2018, and OpenAlex in
  2022<xref ref-type="fn" rid="fn1">1</xref>. Myriad other databases
  exist to support specific bibliometric work or topical analysis;
  PubMed, for example, primarily indexes life sciences and biomedical
  works and The Lens provides a view into innovation through its
  coverage of patent
  knowledge<xref ref-type="fn" rid="fn2">2</xref>.</p>
  <p>The bibliometrics field has long paid attention to the coverage and
  quality of these data sources and investigated differences in
  evaluative outcomes produced from
  them<xref ref-type="fn" rid="fn3">3</xref>. Bruce &amp; Hillmann
  (<xref alt="2004" rid="ref-bruce2004continuum" ref-type="bibr">2004</xref>)
  note that the library community’s efforts to define quality in
  bibliographic records are linked, in part, to their aim to enforce it.
  The advent of OpenAlex- an open database that indexes over 250 million
  scholarly works with broader coverage of the Humanities, non-English
  languages, and the Global South than traditional indexes
  (<xref alt="Priem et al., 2022" rid="ref-priem2022openalex" ref-type="bibr">Priem
  et al., 2022</xref>) generated a new wave of such studies. As an
  aggregator and standardizer of data from Microsoft Academic Graph
  (MAG), Crossref, Open Researcher and Contributor ID (ORCID), Research
  Organization Registry (ROR), Directory of Open Access Journals (DOAJ),
  Unpaywall, PubMed, and several other
  sources<xref ref-type="fn" rid="fn4">4</xref>, its coverage and
  quality have been of interest regarding its efficacy for quantitative
  analysis. This scrutiny has resulted in observations that OpenAlex
  might contain erroneous metadata, or that it is still largely
  incomplete. For example, changing processes for matching and
  disambiguating data have demonstrated the importance of persistent
  identifiers and remaining cautious when creating new open data sources
  dependent on OpenAlex
  (<xref alt="Mongeon et al., 2024" rid="ref-mongeon2024twitter" ref-type="bibr">Mongeon
  et al., 2024</xref>).</p>
  <sec id="coverage-of-openalex-versus-web-of-science">
    <title>Coverage of OpenAlex versus Web of Science</title>
    <p>As a nascent data source, numerous recent studies have compared
    the coverage of OpenAlex to more established databases. Culbert et
    al. (2024) investigated the coverage of reference items between
    OpenAlex, WoS, and Scopus. They found that OpenAlex was comparable
    with commercial databases from an internal reference coverage
    perspective if restricted to a core corpus of publications similar
    to the other two sources, though it lacked cited references. Low
    reference, funder, and affiliation coverage was also found by
    Alonso-Alvarez &amp; Eck
    (<xref alt="2024" rid="ref-alonso2024coverage" ref-type="bibr">2024</xref>),
    though they observed OpenAlex’s coverage of publication and author
    information was high compared to WoS and Scopus. Simard et al.
    (<xref alt="2025" rid="ref-simard2025" ref-type="bibr">2025</xref>)
    and Maddi et al.
    (<xref alt="2024" rid="ref-maddi2024coverage" ref-type="bibr">2024</xref>)
    investigated the OA journal coverage of OpenAlex, WoS, and Scopus
    using the DOAJ and ROAD databases as reference databases. Both
    studies found that OpenAlex indexes more journals and provides more
    balanced geographical coverage. Céspedes et al.
    (<xref alt="2024" rid="ref-cespedes2024linguistic" ref-type="bibr">2024</xref>)
    determined that OpenAlex’s linguistic coverage (75% English) far
    surpassed that of WoS (95% English) from the metadata, with the
    former reduced to 68% upon manual verification of the works
    themselves. However, the language field in OpenAlex is
    algorithmically detected from the title and abstract metadata,
    introducing limitations<xref ref-type="fn" rid="fn5">5</xref>. In a
    coverage comparison of six databases, Ortega &amp; Delgado-Quirós
    (<xref alt="2024" rid="ref-ortega2024retracted" ref-type="bibr">2024</xref>)
    found that OpenAlex indexes more retracted works than WoS, Scopus,
    and PubMed.</p>
  </sec>
  <sec id="metadata-quality-of-openalex-versus-web-of-science">
    <title>Metadata quality of OpenAlex versus Web of Science</title>
    <p>Donner
    (<xref alt="2017" rid="ref-donner2017document" ref-type="bibr">2017</xref>)
    found document type (articles, reviews, letters, and others)
    discrepancies between WoS and journal websites or article full text
    for a sample of 791 publications, with letters and reviews
    particularly affected. Different average page and reference counts
    for errors were also observed for arising discrepancies, hinting at
    the potential development of semi-automatic methods using these
    findings.</p>
    <p>Other studies focused on the quality of OpenAlex data and showed
    that institutional metadata is missing from many OpenAlex records
    (<xref alt="Bordignon, 2024" rid="ref-bordignon2024openalex" ref-type="bibr">Bordignon,
    2024</xref>;
    <xref alt="Zhang et al., 2024" rid="ref-zhang2024institutions" ref-type="bibr">Zhang
    et al., 2024</xref>), and funding metadata is also lacking Schares
    (<xref alt="2024" rid="ref-schares2024funder" ref-type="bibr">2024</xref>).
    Haupka et al.
    (<xref alt="2024" rid="ref-haupka2024types" ref-type="bibr">2024</xref>)
    observed a broader range of materials as research publications in
    OpenAlex compared to Scopus, WoS, and PubMed, potentially explained
    by Ortega &amp; Delgado-Quirós
    (<xref alt="2024" rid="ref-ortega2024retracted" ref-type="bibr">2024</xref>)
    as resulting from the database’s reliance on Crossref’s less
    granular system of classification.</p>
  </sec>
  <sec id="research-objectives">
    <title>Research objectives</title>
    <p>The rise of open science has promoted the creation,
    sustainability, and use of open infrastructure; the completeness and
    accuracy of such sources are compared frequently to proprietary,
    curated databases which tout more exclusive standards for indexing,
    but make their data curation processes less transparent. However, in
    describing how the Citation Index Project (an early version of what
    is now the Web of Science) could be used for evaluation exercises,
    Eugene Garfield emphasized how such information could be used “as a
    basis for comparative studies on the efficacy of various indexing
    techniques”
    (<xref alt="Garfield &amp; Sher, 1963, p. 195" rid="ref-garfield1963citation" ref-type="bibr">Garfield
    &amp; Sher, 1963, p. 195</xref>). Indeed, studies of this kind thus
    serve as “vital statistics” which allude to major implications of
    bibliometric analyses conducted using different data sources. For
    example, Liu et al.
    (<xref alt="2021" rid="ref-liu2021samejournal" ref-type="bibr">2021</xref>)
    found major discrepancies between publication records for the same
    journal indexed in WoS and Scopus, respectively, forcing users into
    the dilemma of Segal’s law<xref ref-type="fn" rid="fn6">6</xref>.
    Thus, this study aims to compare discrepancies between OpenAlex, a
    massive open bibliographic database increasingly utilized by the
    bibliometrics community since its launch, and WoS, an entrenched
    source of data regarded for its high-quality curation and
    standardization and recently found to be the most reliable in terms
    of data completeness and accuracy
    (<xref alt="Singh et al., 2024" rid="ref-singh2024fields" ref-type="bibr">Singh
    et al., 2024</xref>).</p>
    <p>While OpenAlex’s coverage has been under recent intense
    investigation to evaluate its usability for accurate analyses, more
    needs to be understood about the comparability of its metadata to
    other data sources. Our work thus aims to assess the quality of
    metadata in OpenAlex and WoS. More specifically, it addresses the
    following research questions:</p>
    <list list-type="bullet">
      <list-item>
        <p><bold>RQ1</bold>: How frequent are discrepancies between WoS
        and OpenAlex records in terms of document type, language,
        publication year, and number of authors?</p>
      </list-item>
      <list-item>
        <p><bold>RQ3</bold>: What share of records with discrepancies
        are correct in WoS, OpenAlex, neither, or both??</p>
      </list-item>
      <list-item>
        <p><bold>RQ4</bold>: What explains these discrepancies?</p>
      </list-item>
    </list>
    <p>Scientific literature engages with several concepts in various
    combinations in relation to data sources, including
    <italic>quality</italic>, <italic>accuracy</italic>,
    <italic>validity</italic>, <italic>completeness</italic>,
    <italic>reliability</italic>, and <italic>consistency</italic>.
    Bruce and Hillman (2004) describe the most recognized
    characteristics of data quality as completeness, accuracy,
    provenance, conformance to expectations, logical consistency and
    coherence, timeliness, and
    accessibility<xref ref-type="fn" rid="fn7">7</xref>. In this study,
    we largely refer to quality broadly, largely as accuracy and
    completeness, and we consider the relationship of both of these
    concepts in relation to their consistency. Thus, the examination of
    discrepancies reveals not only if and which errors are occurring,
    but also in what frequency. Findings regarding consistency may
    implicate the reliability of the data, potentially proving
    especially useful to those running large-scale analyses. In
    aggregate, this study’s findings may also point to possible reasons
    for such errors which may have useful implication for the validation
    of this data and avoidance of future error through the updating of
    database processes, considering that these errors are difficult to
    identify
    (<xref alt="Meester et al., 2016" rid="ref-meester2016response" ref-type="bibr">Meester
    et al., 2016</xref>).</p>
  </sec>
</sec>
<sec id="data-and-methods">
  <title>Data and methods</title>
  <sec id="data-collection">
    <title>Data collection</title>
    <p>The WoS data used in this study was retrieved from a relational
    database version of the WoS hosted by the Observatoire des sciences
    et des technologies (OST) and limited to the Science Citation Index
    (SCI), the Social Sciences Citation Index (SSCI), and the Arts &amp;
    Humanities Citation Index (A&amp;HCI). We collected all WoS records
    with a DOI published between 2021 and 2023 (N = 7,661,474). We
    removed 30,3094 (0.4%) WoS records with multiple document types to
    avoid complications with the analysis. Of the remaining 7,631,080
    WoS records, 6,599,479 (86.5%) had a DOI match in the February 2024
    snapshot of OpenAlex accessed through Google Big Query (see
    <xref alt="Mazoni &amp; Costas, 2024" rid="ref-mazoni2024" ref-type="bibr">Mazoni
    &amp; Costas, 2024</xref>). We used the subset of 6,594,747 Web of
    Science records with a single DOI match in OpenAlex for our
    analysis. In the OST database, every journal is assigned to one of
    143 specialties and 14 disciplines of the National Science
    Foundation (NSF) classification. For our analysis, we grouped the
    disciplines into four groups: Arts and Humanities (AH), Biomedical
    Research (BM), Natural Science and Engineering (NSE), and Social
    Sciences (SS).</p>
  </sec>
  <sec id="identification-of-discrepancies">
    <title>Identification of discrepancies</title>
    <p>For each matching WoS and OpenAlex record, we compared the
    following four metadata elements: 1) document type, 2) language, 3)
    publication year, and 4) number of authors.</p>
    <p>For the document type, we did not consider discrepancies where a
    record is a review according to WoS and an article according to
    OpenAlex, and vice versa (i.e., we consider articles and reviews as
    the same document type). Furthermore, OpenAlex indexes conference
    papers as articles, and the source type (conference) is meant to
    distinguish them from journal articles. For these reasons, we only
    considered discrepancies for which the record is identified as an
    article or a review in either WoS or OpenAlex, but is identified as
    neither in the other source. We also excluded discrepancies for
    which the record is a meeting abstract in WoS and an article in
    OpenAlex. Overall, we found 427,593 discrepancies that met these
    criteria (6.5% of all records in the dataset).</p>
    <p>The identification of discrepancies in language, publication
    year, and number of authors was limited to the subset of 5,924,459
    articles and reviews with no discrepancy in document type (i.e., the
    records that are articles or reviews in both WoS and OpenAlex). The
    publication language is recorded in its long form (e.g., English) in
    our WoS data and its ISO code (e.g., EN) in OpenAlex, so we created
    a list of all language combinations (e.g., English-EN, English-FR),
    determined which ones constitute discrepancies, and identified
    33,322 discrepancies (0.6% of all articles and reviews) in
    publication language between WoS and OpenAlex. The publication year
    and number of authors being numeric indicators, identifying
    discrepancies in these metadata fields is easily done by calculating
    the difference between the two values and flagging all non-zero
    values. We identified a total of 469,098 discrepancies (7.9% of all
    articles and reviews) in publication years and 70,590 discrepancies
    (1.2% of all articles and reviews) in the number of authors. We
    recorded them both as dichotomous variables (discrepancy vs no
    discrepancy) and as differences between the two values, which
    allowed us to measure both the frequency and the strength of the
    discrepancies.</p>
  </sec>
  <sec id="investigation-of-discrepancies">
    <title>Investigation of discrepancies</title>
    <sec id="discrepancies-sample">
      <title>Discrepancies Sample</title>
      <p>For the publication year and number of authors, we also
      stratified using six bins based on the difference between the
      values in WoS and OpenAlex (-4 or less, -2 to -3, -1, 1, 2 to 3, 4
      or more) to ensure that a range of cases are represented in the
      data.</p>
    </sec>
    <sec id="investigation-process">
      <title>Investigation Process</title>
      <p>The sample discrepancies were manually investigated by looking
      at the article on the journal’s website and the full text when
      necessary and available. We recorded whether the WoS or OpenAlex
      record was correct and, when possible, explained the discrepancy.
      For the publication year, it is typical for articles to have two
      publication dates: the date of the first online publication and
      the date of the publication of the issue. For cases in which the
      online publication year was recorded in one database and the issue
      year was recorded in the other, we did not consider either
      database to be correct.</p>
      <p>For author discrepancies, the landing page and, when available,
      the published version was examined for author counts. Where
      available, author declarations, acknowledgements, and CRediT
      declarations were also considered to clarify authorship where
      groups or consortia were involved in the production of the work.
      For languages, the full text was the primary source, followed by
      the publisher’s landing page for recording observations on
      language assignment. We recorded multiple language full texts if
      available, as well as translated abstracts available on the
      landing page. There are limitations to using the publisher landing
      page with a web browser.  The language encoding in the head of the
      HTML could be used by the GET request to display your preferred
      language type, if the server is configured to respond to such a
      request.</p>
      <p>This process allowed us to go beyond the simple counting of
      differences between the databases and gain insight into the
      different factors that can cause discrepancies and estimate the
      percentage of erroneous records in each database. One caveat, of
      course, is that because our process relies on discrepancies in
      identifying errors, we are not considering cases where both
      databases contain the same error.</p>
    </sec>
  </sec>
</sec>
<sec id="results">
  <title>Results</title>
  <p>We structured our results section by type of discrepancy. For each,
  we first show the distribution of values in the Web of Science and the
  OpenAlex datasets. Note that these are not the distributions in the
  entire Web of Science OpenAlex databases, but the distributions in the
  subset of records with a DOI match between the databases that are
  included in our analysis. Second, we present descriptive statistics
  related to the discrepancies identified and the sample that was used
  for investigations. Finally, we present the results of these
  investigations.</p>
  <sec id="document-type">
    <title>Document type</title>
    <p>Table 2 and Table 3 present the distribution of records across
    document types in WoS and OpenAlex, respectively, to provide a
    general picture of the databases’ content and the differences in
    their classification. While WoS contains twice as many document
    types as OpenAlex, these differences appear mainly among the less
    frequent types, in line with the findings of Haupka et al.
    (<xref alt="2024" rid="ref-haupka2024types" ref-type="bibr">2024</xref>).
    Most documents in both data sources are articles and reviews.</p>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-6-1.png" />
    <p>Tables 4 and 5 show that the vast majority 301,850 of the 310,843
    discrepancies (97%) are cases where a record is an article or review
    in OpenAlex but not WoS. We find that in both tables the majority of
    discrepancies indicate a misclassification of a record as an article
    or review. However, WoS is much more accurate at detecting
    misclassification in OpenAlex (93.5% of the sample) than in WoS
    (68.8% of the sample). This generally points to a much larger number
    of OpenAlex records misclassified as articles or review compared to
    WoS.</p>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-8-1.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-10-1.png" />
  </sec>
  <sec id="publication-language">
    <title>Publication language</title>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-12-1.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-14-1.png" />
    <p>Table 7 shows the discrepancies for articles in English according
    to WoS.</p>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-16-1.png" />
    <p>Table 8 shows the discrepancies for article in English according
    to OpenAlex</p>
    <p>Identifying a ‘correct’ language was not a clear-cut distinction
    in some cases. Multiple language versions, including PDFs, HTML, or
    XML, multi-language abstracts on the landing page or in the work,
    make assigning a single language value at times not possible. We
    found inconsistencies in the language assigned to works from both
    OpenAlex and WoS, with both being correct, even though different
    languages were assigned. We observed that works provided by large
    publishers were more likely to be in English, and local journals
    were more linguistically aligned with the national language, which
    was previously seen in prior studies. Overall, we found that WoS is
    correct more often on language identification than OpenAlex. Wos
    performed better on all disciplines except SS, where OA was more
    successful in identifying the language.</p>
    <p>To understand how an incorrect assignment may have occurred, we
    looked at the abstracts and full text on the landing page as well as
    the full text when it was available. Most sampled works seem to have
    multi-language abstracts either on the landing page or in the PDF of
    the full text. WoS assigned languages were deemed to be correct more
    of the time than OpenAlex. We also found multi-language versions not
    just in PDF, but also in HTML rich text, as well as XML formats. We
    also identified that about one-quarter (51 works) use a translated
    abstract on the landing page but not in the published full text.
    Lastly, we categorized 8 works as single language as they only had
    one language on the landing page and a matching language on the PDF
    full text – they were the exception rather than the norm.</p>
    <p>In the following table, we show the counts of languages
    identified in our sample and the count of works where OA, WoS, or
    both were deemed correct in their assignment. Despite being correct
    less overall, OA seemed to more accurately assign language to
    certain languages, such as Spanish, French, and Portuguese. WoS
    correctly identified all of the Chinese-language papers. Both were
    found to be correct, and this was due to multiple language versions
    of the works being found on the landing page.</p>
    <p>In Table X below, the encoded explanation categories are compared
    with the disciplines to see if they are more frequent in some
    disciplines than others. NSE uses proportionally more multilingual
    abstracts than BM or AH, though very close to SS. Translated
    articles in AH appear much more frequently than in BM, NSE, or SS.
    Single-language articles and multi-language versions of the
    published work were observed least frequently in our sample. Keep in
    mind that our subset includes all the works that were selected based
    on having a difference between what OpenAlex and WoS assigned and
    does not represent the population as a whole.</p>
  </sec>
  <sec id="publication-year">
    <title>Publication year</title>
    <p>Now turning our attention to discrepancies in publication years
    for articles in WoS and OpenAlex, we first present the descriptive
    statistics for publication years</p>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-19-1.png" />
    <sec id="section">
      <title></title>
    </sec>
    <sec id="discrepancies">
      <title>Discrepancies</title>
      <p>The numbr</p>
      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-21-1.png" />
    </sec>
    <sec id="explanation-for-publication-year-discrepancies">
      <title>Explanation for publication year discrepancies</title>
      <p>Publication year was divided between ‘first published year’ and
      ‘issue year’ as both dates do not have to be the same. In the
      majority of works, OpenAlex correctly identified the first
      publication year and WoS assigned the issue year (476 works),
      followed by the opposite condition where WoS used the first
      published year and OpenAlex used the issue year (155 works). The
      remainder of the table includes explanations that identify where
      sources were unclear, not included, do not match, or were
      incorrect, or neither date was available. 255 works did not have a
      publication date, and 11 did not have the issue date.</p>
      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-22-1.png" />
    </sec>
  </sec>
  <sec id="number-of-authors">
    <title>Number of authors</title>
    <p>Author discrepancies can be ambiguous to determine a ‘correct’
    number. Authorships were manually counted on the landing page, and
    where available, the full text, which we considered the
    authoritative number, given that the authors would have reviewed and
    approved it. Additionally, Crossref metadata for authors was also
    considered as a possible explanation, particularly as OpenAlex draws
    heavily from this data source. The following table contains
    explanations determined by the investigators as to whether the
    OpenAlex or WoS count was correct, and if not, why the information
    might not be available. In rare cases, some articles contained over
    1000 authors and were set aside for the time as being ‘too many to
    count’ manually.</p>
    <p>In Table 6, the majority were found to have OpenAlex as the
    correct author reporting at 594 counts out of 1107 (53.7%), and WoS
    author counts were correct 368 (33.2%). Neither was found to be
    correct 91 times (8.2%). Bad URLs (meaning they are non-resolving,
    behind a paywall with no landing page, or were links to XML pages
    meant for similarity checking, among other possibilities) were found
    2.3% of the time, followed by the lack of landing pages for works
    1.4% of the time. Only 2 (0.2%) were found to be unregistered at the
    time, and only 1 (0.1%) was shown as ‘withdrawn’ by the publisher.
    In Table 7, author count explanations are examined regarding
    disciplines.</p>
    <p>There are no large discrepancies biased towards one discipline or
    the other. Social Sciences had slightly more correct authors
    assigned by OpenAlex than the other disciplines, whereas BM had
    slightly more attributions by WoS as correct. BM also had more where
    neither was correct, followed by NSE, which is to be expected as
    both of these disciplines typically have more consortia, research
    groups, or teams attributed as authors, which can be tricky to
    determine if not explicitly declared by the authors. AH and NSE had
    more bad URLs, and AH contained the majority of those with no
    landing page for the work, making it beyond the scope of our
    methods. NSE had the majority of those over 1000 authors, which is
    not surprising given the large teams in fields such as Physics.</p>
  </sec>
</sec>
<sec id="discussion">
  <title>Discussion</title>
  <p>The value of bibliographic data sources is derived from different
  elements, including their coverage, completeness, and data accuracy
  (Visser et al., 2021). This research contributes to understanding the
  value and utility of OpenAlex as a data source by investigating its
  metadata discrepancies concerning document type. Our findings support
  those of past studies that have found metadata quality in OpenAlex to
  be in need of improvement lower in comparison to WoS. As Haupka et
  al. (2024) have indicated, document type classifications are critical
  for bibliometric research and evaluation, and so improved accuracy of
  document types is desired. We also echo that calibrating diverse
  document types across disciplines and different bibliographic
  databases remains a significant challenge (Haupka et al., 2024). For
  the next stage of this research, we will include additional metadata
  elements widely used in bibliometric analyses and investigate
  disciplinary differences in metadata quality. Further research will
  examine how metadata quality issues in OpenAlex could affect journal
  and institutional-level metrics and, thus, the results of
  institutional rankings like the open edition of the Leiden
  Ranking.</p>
  <sec id="document-type-discrepanciesmh1">
    <title>Document-type
    discrepancies<xref alt="[MH1]" rid="_msocom_1">[MH1]</xref> </title>
  </sec>
  <sec id="publication-year-discrepanciesmh2">
    <title>Publication year
    discrepancies<xref alt="[MH2]" rid="_msocom_2">[MH2]</xref> </title>
    <p>Pinning down a single published date value can possibly yield
    different dates that are valid. To gain a little more insight into
    the sources for both OpenAlex and WoS, we examined an API
    call<xref alt="[1]" rid="_ftn1">[1]</xref> for all 10 dates
    available in the Crossref metadata (posted, issued, updated-to,
    approved, indexed, accepted, published, published-print,
    published-online, deposited – three of these have to do with the
    published date). As an example, the DOI 10.1093/isle/isaa156 has
    three different unique dates that align with the OA and WoS values.
    The published and the published-online dates are 2020-11-30, and
    this agrees with the OpenAlex published date. But the
    published-print is 2021-12-13, and this agrees with the WoS values.
    So, both are correct. Yet, this work’s metadata was not deposited in
    Crossref until 2024-08-16. A single date, then, is tricky to pin
    down and does not completely capture the journey of a work through
    its scholarly lifecycle.</p>
    <p>Overall, we found that published date values were preferred by
    OpenAlex, and WoS tends to use the issue date (476 works). 155 works
    were found that WoS used the published date, and OA used the issue
    date. The total of these (631 works) accounts for 67% of the works
    sampled, and the remainder have an omission, incorrect value, or the
    source was unclear with the date provided by either WoS or OpenAlex.
    Further work is needed to understand how local practices within
    publishers or other limitations may be affecting how publishers
    provide this information and make it available to scholarly
    databases.</p>
  </sec>
  <sec id="language-discrepanciesmh3-mh4-mh5">
    <title>Language
    discrepancies<xref alt="[MH3]" rid="_msocom_3">[MH3]</xref> <xref alt="[MH4]" rid="_msocom_4">[MH4]</xref> <xref alt="[MH5]" rid="_msocom_5">[MH5]</xref> </title>
    <p>Reducing the language down to a single value for analysis is
    challenging and comes with many caveats. We observed that
    multi-language abstracts are not uncommon, though it is unknown if
    this is increasing.</p>
    <p>Overall, there is an inconsistency in the single language value
    that is limited by both the OpenAlex and WoS metadata schema as many
    versions had multiple languages in the landing page, abstracts, and
    in the full text. This makes declaring a single language assignment
    difficult or even misrepresenting.</p>
  </sec>
  <sec id="author-discrepanciesmh6-mh7">
    <title>Author
    discrepancies<xref alt="[MH6]" rid="_msocom_6">[MH6]</xref> <xref alt="[MH7]" rid="_msocom_7">[MH7]</xref> </title>
    <p>The author discrepancies subset was selected based on those in
    the dataset that had a difference between ‘oa_n_authors’ and
    ‘wos_n_authors’. As such, we find a few publishers that are
    over-represented as their local practices or limitations affect
    whether authors are entered into the metadata at all.</p>
    <p>Overall, we see that OpenAlex was most correct on author counts
    on 53.7% of the subset of works, whereas WoS was correct on 33.2% of
    the subset. Regarding disciplines, the distribution is fairly even
    across disciplines for both OpenAlex and Wos, but BM and NSE are
    more affected by the ‘neither is correct’ condition, which is most
    likely affected by those authorships using consortia or other group
    identifiers. We also looked to see if there were similarities
    between OpenAlex and Crossref author counts and found that 50.3% of
    works contained the same authors, but alarmingly, we also found that
    25.3% had author list errors, and 14.4% contained no authors at all.
    These omissions account for a large proportion of the works, and
    while both OpenAlex and WoS have other methods for extracting
    authors from landing pages and full text, publisher-supplied
    metadata should align with the published work.</p>
    <p>The use of group terminology for authorships, such as consortia,
    research groups, study groups, teams, etc., seems to be common in
    the BM and NSE fields. There is great inconsistency in how these
    groups are accounted for, credit is given for authorship, how
    members are identified, or the language used to signify how authors
    work on behalf of or as part of a consortium. A few works in BM
    utilized the CRediT declaration and provided complete author lists,
    which clearly defined who was an author/contributor to the work. We
    also observed that group names may be listed as authors on the first
    page of the work or the landing page but were absent from the CRediT
    declaration. While the CRediT declaration made our job easier and
    established a clear authority by the authors on contributions, it is
    applied inconsistently. More work needs to be done to document these
    local practices to enable or encourage more consistency in author
    reporting and responsibilities.</p>
    <p>We also observed consistently that OpenAlex excludes organization
    names as contributors, even if they exist in the Crossref metadata.
    Their process does not seem to affect singular names, such as those
    used in non-Western naming conventions, but it does seem to exclude
    consortia and other group identifiers in the author listing.</p>
    <p>Conversely, WoS seems to be consistently overcounting author
    names, even if the Crossref metadata seems to align with the
    published version of the work. In contrast to the ‘lazy’ method of
    OpenAlex by excluding organization/group names, any pattern matching
    used by WoS seems to be greedy, accepting many more than there are
    listed in the work or author declarations. This seems to work, as
    most of the times that WoS was correct on the author count, the
    Crossref metadata was incorrect.</p>
    <p>Translated works were an exception to the greedy approach by WoS,
    as their counts consistently excluded the number of translators on a
    work, mostly affecting works in the AH discipline. OpenAlex included
    both the author and translator on works, and this aligned with the
    Crossref metadata that also included translators as
    authors/contributors.</p>
  </sec>
  <sec id="advantages-and-disadvantages-in-using-each-sourcemh8">
    <title>Advantages and disadvantages in using each
    source<xref alt="[MH8]" rid="_msocom_8">[MH8]</xref> </title>
    <p>Haunschild and Bornmann (2024) offer useful insights from their
    use of OpenAlex data to produce overlay maps, one being that the
    main advantage of using OpenAlex data is that it can be used without
    restrictions (i.e., free availability). These are offset by
    concurrent limitations, such as in partial correctness or erroneous
    metadata, in their case, in OpenAlex assigned
    “concepts”<xref alt="[2]" rid="_ftn2">[2]</xref> raising questions
    about the reliability and validity of OpenAlex’s processes
    (Haunschild &amp; Bornmann, 2024).</p>
  </sec>
</sec>
<sec id="conclusionmh9-mh10">
  <title>Conclusion<xref alt="[MH9]" rid="_msocom_9">[MH9]</xref> <xref alt="[MH10]" rid="_msocom_10">[MH10]</xref> </title>
  <p>The idiosyncrasies of databases should be accounted for when
  researchers or research evaluators decide which to use for their data
  analysis tasks, as they affect the discoverability of works as well as
  metrics (Barbour et al., 2025). Differences among data sources
  regarding indexing coverage are, to a degree, largely understood and
  part of decision-making, Barbour et al. (2025) explain how the known
  selectivity of WoS and Scopus contrast with OpenAlex, which has a
  global and comprehensive aim. Metadata discrepancies, however, are
  more granular peculiarities and difficult to track.</p>
  <p>The Paris Conference on Open Research Information and the Barcelona
  Declaration on Open Research Information are two initiatives that
  emphasize the need for and normalization of open research information.
  The Barcelona Declaration called for signatories to work with systems
  that support open research information (Barcelona Declaration, 2024).
  With the tide turning toward open data sources and researchers and
  institutions embracing OpenAlex and other open data sources and tools,
  more research will be needed on the quality and coverage of OpenAlex
  and the other data sources it depends on. This carries implications
  for OpenAlex, as they look to the research community for feedback on
  necessary improvements to their metadata, as well as for those
  conducting research and research evaluation using open sources, who
  must remain apprised of findings related to limitations of the data
  they use.</p>
  <sec id="limitations">
    <title>Limitations</title>
  </sec>
  <sec id="future-work">
    <title>Future work</title>
    <p>Many open bibliographic data sources such as OpenAlex are
    downstream from metadata providers such as Crossref; as a crucial
    source for PubMed, OpenAIRE, OpenCitations, and, among other
    metadata digital infrastructure organizations, attention should be
    focused on the provenance of metadata, and their sustainability and
    reliability as a data stream for scholarly metadata (van Eck &amp;
    Waltman, 2025). In noting that the quality of bibliographic metadata
    is quantifiable and measurable, Bruce and Hillman (2004) also note
    that enforcing standards must be done at the level of the community.
    Liu et al. (2021) make recommendations for remedying discrepancies:
    they recommend the use of official publication dates to avoid
    different publication years; this would also require the alignment
    of policies across databases. They also suggest databases work
    closely with journal publishers to mitigate document omission and
    metadata errors, double checking them frequently (Liu et al., 2021).
    Finally, they recommend the examination of data processing flows to
    identify potential causes when it comes to duplicate entries (Liu et
    al., 2021). To this list we also add…</p>
  </sec>
</sec>
<sec id="conflicts-of-interest">
  <title>Conflicts of interest</title>
  <p>The authors have no conflicts of interest to report.</p>
</sec>
<sec id="author-contributions">
  <title>Author contributions</title>
  <p><ext-link ext-link-type="uri" xlink:href="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</ext-link>
  (PM),
  <ext-link ext-link-type="uri" xlink:href="http://credit.niso.org/contributor-roles/data-curation/">Data
  curation</ext-link> (PM),
  <ext-link ext-link-type="uri" xlink:href="http://credit.niso.org/contributor-roles/formal-analysis/">Formal
  analysis</ext-link> (PM, PR), Investigation (PM, MH, GK, RM, PR, RT,
  SW),
  <ext-link ext-link-type="uri" xlink:href="http://credit.niso.org/contributor-roles/project-administration/">Project
  administration</ext-link> (PM), (PM),
  <ext-link ext-link-type="uri" xlink:href="http://credit.niso.org/contributor-roles/visualization/">Visualization</ext-link>
  (RT, PR),
  <ext-link ext-link-type="uri" xlink:href="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing
  – original draft</ext-link> (PM, MH, PR),
  <ext-link ext-link-type="uri" xlink:href="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing
  – review &amp; editing</ext-link> (PM, MH, GK, RM, PR, RT, SW)</p>
</sec>
<sec id="data-availability">
  <title>Data availability</title>
</sec>
<sec id="references">
  <title>References</title>
</sec>
<sec id="notes">
  <title>Notes</title>
  <p><xref alt="[1]" rid="_ftnref1">[1]</xref>
  <ext-link ext-link-type="uri" xlink:href="https://api.crossref.org/works/?select=DOI,posted,issued,update-to,approved,indexed,accepted,published,published-print,published-online,deposited&amp;filter=doi:10.1093/isle/isaa156">https://api.crossref.org/works/?select=DOI,posted,issued,update-to,approved,indexed,accepted,published,published-print,published-online,deposited&amp;filter=doi:10.1093/isle/isaa156</ext-link>
  This API call selects all 10 date elements available in the Crossref
  REST API.</p>
  <p><xref alt="[2]" rid="_ftnref2">[2]</xref> OpenAlex Concepts have
  been deprecated in favor of Topics:
  <ext-link ext-link-type="uri" xlink:href="https://docs.openalex.org/api-entities/concepts">https://docs.openalex.org/api-entities/concepts</ext-link></p>
  <p> <xref alt="[MH1]" rid="_msoanchor_1">[MH1]</xref><ext-link ext-link-type="uri" xlink:href="https://link.springer.com/article/10.1007/s11192-017-2483-y#Sec6"><bold>https://link.springer.com/article/10.1007/s11192-017-2483-y#Sec6</bold></ext-link></p>
  <p><bold>VERY useful for doc types.</bold></p>
  <p> <xref alt="[MH2]" rid="_msoanchor_2">[MH2]</xref><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1505.00796">https://arxiv.org/abs/1505.00796</ext-link></p>
  <p> <xref alt="[MH3]" rid="_msoanchor_3">[MH3]</xref>From Shi et
  al. (2025): The lack of standardized and widely adopted Romanization
  schemes for many languages</p>
  <p>itself results in errors and inconsistencies: localized standards
  may be developed and used</p>
  <p>in isolation; when multiple schemes exist like this, guidance may
  be referenced and applied</p>
  <p>inconsistently (Park, 2007); or Romanized forms may be decided on
  independent of any guidance.</p>
  <p>Moreover, the choice to record Romanizations only may preclude
  access to resources</p>
  <p>by users unfamiliar with such schemes or who would transcribe or
  transliterate differently</p>
  <p>(Rigby, 2015). This raises further ethical questions about who
  metadata caters to when rendered</p>
  <p>only in translation, transcription, or transliteration.d</p>
  <p>·        
   <xref alt="[MH4]" rid="_msoanchor_4">[MH4]</xref>Vera-Baceta, M.-A.,
  Thelwall, M., &amp; Kousha, K. (2019). Web of Science and Scopus
  language coverage. <italic>Scientometrics</italic>,
  <italic>121</italic>(3), 1803–1813.
  <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11192-019-03264-z">https://doi.org/10.1007/s11192-019-03264-z</ext-link>
  English is by far the dominant language in both Scopus (at &gt;92%)
  and WoS (&gt;95%). Generally, WoS has less coverage of most
  (non-English) languages than Scopus, and in particular, has less than
  1/10 as many works in Chinese (which comes in second, at 2.76%, in
  Scopus). WoS does have higher coverage of some languages, including
  Spanish. These numbers differ somewhat from studies before 2015, as
  WoS has since added the Emerging Sources Citation Index to its core
  collection, expanding coverage of non-English works.</p>
  <p>·         Language coverage varies widely across subject areas.</p>
  <p>·        
   <xref alt="[MH5]" rid="_msoanchor_5">[MH5]</xref>Cespedes:
  Find that even accounting for errors, works in OpenAlex have far
  greater linguistic diversity than Web of Science. WoS is more than 95%
  English, whereas the OpenAlex sample is identified as such by the
  metadata 75% of the time.</p>
  <p>·         Manual examination of the works themselves (vs. metadata)
  brings this number closer to 68%. English is thus overestimated, as
  are other Western European languages, including French and German.
  Chinese and Russian are underreported.</p>
  <p>·         The language field is populated for 98% of the documents,
  and is true to the underlying document for more than 85%.</p>
  <p>·         Lack or presence of DOIs does not seem to impact accuracy
  of language.</p>
  <p>·         Point out that the OpenAlex language field is
  algorithmically derived from the metadata (title &amp; abstract), and
  that the <italic>langdetect</italic> algorithm is limited to 55
  languages.</p>
  <p> <xref alt="[MH6]" rid="_msoanchor_6">[MH6]</xref>Strotmann, A.,
  &amp; Zhao, D. (2012). Author name disambiguation: What difference
  does it make in author‐based citation analysis? Journal of the
  American Society for Information Science and Technology, 63(9),
  1820–1833. https://doi.org/10.1002/asi.22695</p>
  <p>·          <xref alt="[MH7]" rid="_msoanchor_7">[MH7]</xref>Without
  complex author disambiguation, even in a single field (stem cell
  research) over a short period (2004-2009), more than a third of “top
  authors” wind up being conflated groups of hundreds of individuals
  sharing a last name + first initial. Chinese and Korean names are
  particularly affected by this. The differences between author
  co-citation analyses with and without advanced disambiguation is
  significant when such authors are present, and the effect is generally
  a complex one, rather than simply the conflation of pairs of
  authors.</p>
  <p>·         Mentions that Web of Science could be a problem, given
  the possibility of omitting the last author (which is generally easier
  to disambiguate, given the lower number of name collisions, and lower
  prevalence of Chinese/Korean names in this position).</p>
  <p> <xref alt="[MH8]" rid="_msoanchor_8">[MH8]</xref><ext-link ext-link-type="uri" xlink:href="https://osf.io/preprints/metaarxiv/smxe5_v2">https://osf.io/preprints/metaarxiv/smxe5_v2</ext-link></p>
  <p> <xref alt="[MH9]" rid="_msoanchor_9">[MH9]</xref>Oncepossibledatabaseerrorsareidentified,theycanbenotifiedtothedatabasestaffthroughdedicatedsupport/feedback
  mechanisms. We have noticed that Scopus and WoS are both very
  responsive to these feedbacks (Meester et al., 2016). As regards
  database administrators, at the risk of being repetitive, we renew our
  exhortation to improve in terms of data cleaning. We remark that all
  the database errors analyzed and classified in this research were
  preventable: in fact, all the citations omitted by one database are,
  by definition, correctly indexed by the other one.
  Weareawarethatthecitation-matchingalgorithmsusedbydatabaseswillneverbeinfallible,astheystruggletofindthe
  optimalbalancebetween(i)theriskoffailingtoidentifyauthenticcitations(falsenegatives)and(ii)thatofassigningphantom
  citations (false positives). Nevertheless, we believe that databases
  could introduce additional (automated) controls on the results of the
  citation mapping process (e.g., not to reinvent the wheel, the
  automated algorithm presented in (Franceschini et al., 2013)). This
  would be much more effective than waiting for the feedbacks from
  users, with an important benefit in terms of image. In the interest of
  the entire scientific community and their own one, we hope that Scopus
  and WoS will invest in such improvements.</p>
</sec>
</body>

<back>
<ref-list>
  <title></title>
  <ref id="ref-alonso2024coverage">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Alonso-Alvarez</surname><given-names>P</given-names></name>
        <name><surname>Eck</surname><given-names>N J van</given-names></name>
      </person-group>
      <article-title>Coverage and metadata availability of african publications in OpenAlex: A comparative analysis</article-title>
      <source>arXiv preprint arXiv:2409.01120</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://doi.org/10.48550/arXiv.2409.01120</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2409.01120</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-bruce2004continuum">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Bruce</surname><given-names>Thomas R.</given-names></name>
        <name><surname>Hillmann</surname><given-names>Diane I.</given-names></name>
      </person-group>
      <article-title>The continuum of metadata quality: Defining, expressing, exploiting</article-title>
      <source>Metadata in practice</source>
      <person-group person-group-type="editor">
        <name><surname>Hillmann</surname><given-names>Diane I.</given-names></name>
        <name><surname>Westbrooks</surname><given-names>Elaine L.</given-names></name>
      </person-group>
      <publisher-name>ALA Editions</publisher-name>
      <publisher-loc>Chicago</publisher-loc>
      <year iso-8601-date="2004">2004</year>
      <fpage>238</fpage>
      <lpage>256</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bordignon2024openalex">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Bordignon</surname><given-names>F</given-names></name>
      </person-group>
      <article-title>Is OpenAlex a revolution or a challenge for bibliometrics/bibliometricians?</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://enpc.hal.science/hal-04520837</uri>
    </element-citation>
  </ref>
  <ref id="ref-cespedes2024linguistic">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Céspedes</surname><given-names>L</given-names></name>
        <name><surname>Kozlowski</surname><given-names>D</given-names></name>
        <name><surname>Pradier</surname><given-names>C</given-names></name>
        <name><surname>Sainte-Marie</surname><given-names>M H</given-names></name>
        <name><surname>Shokida</surname><given-names>N S</given-names></name>
        <name><surname>Benz</surname><given-names>P</given-names></name>
        <name><surname>Poitras</surname><given-names>C</given-names></name>
        <name><surname>Ninkov</surname><given-names>A B</given-names></name>
        <name><surname>Ebrahimy</surname><given-names>S</given-names></name>
        <name><surname>Ayeni</surname><given-names>P</given-names></name>
        <name><surname>Filali</surname><given-names>S</given-names></name>
        <name><surname>Li</surname><given-names>B</given-names></name>
        <name><surname>Larivière</surname><given-names>V</given-names></name>
      </person-group>
      <article-title>Evaluating the linguistic coverage of OpenAlex: An assessment of metadata accuracy and completeness</article-title>
      <source>arXiv preprint arXiv:2409.10633</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://doi.org/10.48550/arXiv.2409.10633</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2409.10633</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-clarivate_history">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Clarivate</surname></name>
      </person-group>
      <article-title>The history of ISI and the work of eugene garfield</article-title>
      <source>The Institute for Scientific Information</source>
      <uri>https://clarivate.com/academia-government/the-institute-for-scientific-information/history/</uri>
    </element-citation>
  </ref>
  <ref id="ref-donner2017document">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Donner</surname><given-names>P</given-names></name>
      </person-group>
      <article-title>Document type assignment accuracy in the journal citation index data of web of science</article-title>
      <source>Scientometrics</source>
      <year iso-8601-date="2017">2017</year>
      <volume>113</volume>
      <uri>https://doi.org/10.1007/s11192-017-2483-y</uri>
      <pub-id pub-id-type="doi">10.1007/s11192-017-2483-y</pub-id>
      <fpage>219</fpage>
      <lpage>236</lpage>
    </element-citation>
  </ref>
  <ref id="ref-garfield1963citation">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Garfield</surname><given-names>E</given-names></name>
        <name><surname>Sher</surname><given-names>I H</given-names></name>
      </person-group>
      <article-title>New factors in the evaluation of scientific literature through citation indexing</article-title>
      <source>American Documentation</source>
      <year iso-8601-date="1963">1963</year>
      <volume>14</volume>
      <issue>3</issue>
      <uri>https://doi.org/10.1002/asi.5090140304</uri>
      <pub-id pub-id-type="doi">10.1002/asi.5090140304</pub-id>
      <fpage>195</fpage>
      <lpage>201</lpage>
    </element-citation>
  </ref>
  <ref id="ref-haupka2024types">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Haupka</surname><given-names>N</given-names></name>
        <name><surname>Culbert</surname><given-names>J H</given-names></name>
        <name><surname>Schniedermann</surname><given-names>A</given-names></name>
        <name><surname>Jahn</surname><given-names>N</given-names></name>
        <name><surname>Mayr</surname><given-names>P</given-names></name>
      </person-group>
      <article-title>Analysis of the publication and document types in OpenAlex, web of science, scopus, pubmed and semantic scholar</article-title>
      <source>arXiv preprint arXiv:2406.15154</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://doi.org/10.48550/arXiv.2406.15154</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2406.15154</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-liu2021samejournal">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>W</given-names></name>
        <name><surname>Huang</surname><given-names>M</given-names></name>
        <name><surname>Wang</surname><given-names>H</given-names></name>
      </person-group>
      <article-title>Same journal but different numbers of published records indexed in scopus and web of science core collection: Causes, consequences, and solutions</article-title>
      <source>Scientometrics</source>
      <year iso-8601-date="2021">2021</year>
      <volume>126</volume>
      <issue>5</issue>
      <uri>https://doi.org/10.1007/s11192-021-03934-x</uri>
      <pub-id pub-id-type="doi">10.1007/s11192-021-03934-x</pub-id>
      <fpage>4541</fpage>
      <lpage>4550</lpage>
    </element-citation>
  </ref>
  <ref id="ref-maddi2024coverage">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Maddi</surname><given-names>A</given-names></name>
        <name><surname>Maisonobe</surname><given-names>M</given-names></name>
        <name><surname>Boukacem-Zeghmouri</surname><given-names>C</given-names></name>
      </person-group>
      <article-title>Geographical and disciplinary coverage of open access journals: OpenAlex, scopus and WoS</article-title>
      <source>arXiv preprint arXiv:2411.03325</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://doi.org/10.48550/arXiv.2411.03325</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2411.03325</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-meester2016response">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Meester</surname><given-names>W J N</given-names></name>
        <name><surname>Colledge</surname><given-names>L</given-names></name>
        <name><surname>Dyas</surname><given-names>E E</given-names></name>
      </person-group>
      <article-title>A response to “the museum of errors/horrors in scopus” by franceschini et al</article-title>
      <source>Journal of Informetrics</source>
      <year iso-8601-date="2016">2016</year>
      <volume>10</volume>
      <issue>2</issue>
      <uri>https://doi.org/10.1016/j.joi.2016.04.011</uri>
      <pub-id pub-id-type="doi">10.1016/j.joi.2016.04.011</pub-id>
      <fpage>569</fpage>
      <lpage>570</lpage>
    </element-citation>
  </ref>
  <ref id="ref-mongeon2024twitter">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Mongeon</surname><given-names>P</given-names></name>
        <name><surname>Bowman</surname><given-names>T</given-names></name>
        <name><surname>Costas</surname><given-names>R</given-names></name>
        <name><surname>Arroyo-Machado</surname><given-names>W</given-names></name>
      </person-group>
      <article-title>An update on the scholars on twitter dataset</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://www.leidenmadtrics.nl/articles/an-update-on-the-scholars-on-twitter-dataset</uri>
    </element-citation>
  </ref>
  <ref id="ref-narin1976evaluative">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Narin</surname><given-names>F</given-names></name>
      </person-group>
      <source>Evaluative bibliometrics: The use of publication and citation analysis in the evaluation of scientific activity</source>
      <publisher-name>Computer Horizons Washington, D. C.</publisher-name>
      <year iso-8601-date="1976">1976</year>
    </element-citation>
  </ref>
  <ref id="ref-ortega2024retracted">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ortega</surname><given-names>J L</given-names></name>
        <name><surname>Delgado-Quirós</surname><given-names>L</given-names></name>
      </person-group>
      <article-title>The indexation of retracted literature in seven principal scholarly databases: A coverage comparison of dimensions, OpenAlex, PubMed, scilit, scopus, the lens and web of science</article-title>
      <source>Scientometrics</source>
      <year iso-8601-date="2024">2024</year>
      <volume>129</volume>
      <issue>7</issue>
      <uri>https://doi.org/10.1007/s11192-024-05034-y</uri>
      <pub-id pub-id-type="doi">10.1007/s11192-024-05034-y</pub-id>
      <fpage>3769</fpage>
      <lpage>3785</lpage>
    </element-citation>
  </ref>
  <ref id="ref-priem2022openalex">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Priem</surname><given-names>J</given-names></name>
        <name><surname>Piwowar</surname><given-names>H</given-names></name>
        <name><surname>Orr</surname><given-names>R</given-names></name>
      </person-group>
      <article-title>OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts</article-title>
      <source>arXiv preprint arXiv:2205.01833</source>
      <year iso-8601-date="2022">2022</year>
      <uri>https://doi.org/10.48550/arxiv.2205.01833</uri>
      <pub-id pub-id-type="doi">10.48550/arxiv.2205.01833</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-schares2024funder">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schares</surname><given-names>E</given-names></name>
      </person-group>
      <article-title>Comparing funder metadata in OpenAlex and dimensions</article-title>
      <source>Iowa State University</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://doi.org/10.31274/b8136f97.ccc3dae4</uri>
      <pub-id pub-id-type="doi">10.31274/b8136f97.ccc3dae4</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-singh2024fields">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Singh</surname><given-names>P</given-names></name>
        <name><surname>Singh</surname><given-names>V K</given-names></name>
        <name><surname>Kanaujia</surname><given-names>A</given-names></name>
      </person-group>
      <article-title>Exploring the publication metadata fields in web of science, scopus and dimensions: Possibilities and ease of doing scientometric analysis</article-title>
      <source>Journal of Scientometric Research</source>
      <year iso-8601-date="2024">2024</year>
      <volume>13</volume>
      <issue>3</issue>
      <uri>https://doi.org/10.5530/jscires.20041144</uri>
      <pub-id pub-id-type="doi">10.5530/jscires.20041144</pub-id>
      <fpage>715</fpage>
      <lpage>731</lpage>
    </element-citation>
  </ref>
  <ref id="ref-zhang2024institutions">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zhang</surname><given-names>L</given-names></name>
        <name><surname>Cao</surname><given-names>Z</given-names></name>
        <name><surname>Shang</surname><given-names>Y</given-names></name>
        <name><surname>Sivertsen</surname><given-names>G</given-names></name>
        <name><surname>Huang</surname><given-names>Y</given-names></name>
      </person-group>
      <article-title>Missing institutions in OpenAlex: Possible reasons, implications, and solutions</article-title>
      <source>Scientometrics</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://doi.org/10.1007/s11192-023-04923-y</uri>
      <pub-id pub-id-type="doi">10.1007/s11192-023-04923-y</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-simard2025">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Simard</surname><given-names>Marc-André</given-names></name>
        <name><surname>Basson</surname><given-names>Isabel</given-names></name>
        <name><surname>Hare</surname><given-names>Madelaine</given-names></name>
        <name><surname>Larivière</surname><given-names>Vincent</given-names></name>
        <name><surname>Mongeon</surname><given-names>Philippe</given-names></name>
      </person-group>
      <article-title>Examining the geographic and linguistic coverage of gold and diamond open access journals in OpenAlex, scopus and web of science</article-title>
      <source>Quantitative Science Studies</source>
      <year iso-8601-date="2025-05-07">2025</year><month>05</month><day>07</day>
      <uri>https://doi.org/10.1162/QSS.a.1</uri>
      <pub-id pub-id-type="doi">10.1162/QSS.a.1</pub-id>
      <fpage>1</fpage>
      <lpage>29</lpage>
    </element-citation>
  </ref>
  <ref id="ref-segal_law_wikipedia">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Wikipedia contributors</string-name>
      </person-group>
      <article-title>Segal’s law</article-title>
      <publisher-name>https://en.wikipedia.org/wiki/Segal%27s_law; Wikipedia, The Free Encyclopedia</publisher-name>
      <year iso-8601-date="2024">2024</year>
    </element-citation>
  </ref>
  <ref id="ref-mazoni2024">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Mazoni</surname><given-names>Alysson</given-names></name>
        <name><surname>Costas</surname><given-names>Rodrigo</given-names></name>
      </person-group>
      <article-title>Towards the democratisation of open research information for scientometrics and science policy: the Campinas experience</article-title>
      <year iso-8601-date="2024-06-06">2024</year><month>06</month><day>06</day>
      <uri>https://www.leidenmadtrics.nl/articles/towards-the-democratisation-of-open-research-information-for-scientometrics-and-science-policy-the-campinas-experience</uri>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p><ext-link ext-link-type="uri" xlink:href="https://www.webofscience.com/wos/">https://www.webofscience.com/wos/</ext-link>
    (originally named the Science Citation Index);
    <ext-link ext-link-type="uri" xlink:href="https://www.scopus.com/">https://www.scopus.com/</ext-link>;
    <ext-link ext-link-type="uri" xlink:href="https://www.crossref.org/">https://www.crossref.org/</ext-link>; 
    <ext-link ext-link-type="uri" xlink:href="https://scholar.google.com/">https://scholar.google.com/</ext-link>; 
    <ext-link ext-link-type="uri" xlink:href="https://www.dimensions.ai/">https://www.dimensions.ai/</ext-link>; 
    <ext-link ext-link-type="uri" xlink:href="https://openalex.org/">https://openalex.org/</ext-link>.</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p><ext-link ext-link-type="uri" xlink:href="https://pubmed.ncbi.nlm.nih.gov/">https://pubmed.ncbi.nlm.nih.gov/</ext-link>;
    <ext-link ext-link-type="uri" xlink:href="https://www.lens.org/">https://www.lens.org/</ext-link>.</p>
  </fn>
  <fn id="fn3">
    <label>3</label><p>Turgel and Chernova (2024) usefully point out
    that “citation”, “bibliometric”, and “Scientometric” databases are
    all distinct. Citation databases focus on citation connections
    between documents; similarly, bibliometric databases focus on
    publications and their citation metrics. Scientometric databases
    possess a range of metadata and indicators for more robust analyses
    of the structure and dynamics or the research landscape. They define
    Web of Science and OpenAlex as Scientometric databases (pg. 4).</p>
  </fn>
  <fn id="fn4">
    <label>4</label><p>See OpenAlex’s data sources page:
    <ext-link ext-link-type="uri" xlink:href="https://help.openalex.org/hc/en-us/articles/24397285563671-About-the-data">https://help.openalex.org/hc/en-us/articles/24397285563671-About-the-data</ext-link></p>
  </fn>
  <fn id="fn5">
    <label>5</label><p><ext-link ext-link-type="uri" xlink:href="https://docs.openalex.org/api-entities/works/work-object#language">https://docs.openalex.org/api-entities/works/work-object#language</ext-link></p>
  </fn>
  <fn id="fn6">
    <label>6</label><p>Segal’s law states that “A man with a watch knows
    what time it is. A man with two watches is never sure”
    (<xref alt="Wikipedia contributors, 2024" rid="ref-segal_law_wikipedia" ref-type="bibr">Wikipedia
    contributors, 2024</xref>)</p>
  </fn>
  <fn id="fn7">
    <label>7</label><p>See Bruce &amp; Hillmann
    (<xref alt="2004" rid="ref-bruce2004continuum" ref-type="bibr">2004</xref>)
    for a definition of each of these characteristics.</p>
  </fn>
</fn-group>
</back>


</article>