---
title: "Investigating metadata discrepancies between OpenAlex and the Web of Science: the case of Library and Information Sciences"
# date: 2024-11-12
authors:
  - name: Philippe Mongeon
    orcid: 0000-0003-1021-059X
    email: PMongeon@dal.ca
    corresponding: true
  - name: Madelaine Hare
    orcid: 0000-0003-1021-059X
    corresponding: false
  - name: Geoff Krause
    orcid: 0000-0003-1021-059X
    corresponding: false
  - name: Rebecca Marjoram
    orcid: 0000-0003-1021-059X
    corresponding: false
  - name: Poppy Riddle
    orcid: 0000-0003-1021-059X
    corresponding: false
  - name: Remi Toupin
    orcid: 0000-0003-1021-059X
    corresponding: false
  - name: Summer Wilson
    orcid: 0000-0003-1021-059X
    corresponding: false
bibliography: references.bib
csl: apa.csl
code-overflow: wrap
code-tools: 
  source: FALSE
  toggle: TRUE
abstract: "Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors.  By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes."
format:
  wordcount-html: 
    wordcount-banner: true
  html:
    comments:
      hypothesis: false
  docx: default
#  jats: default
#  pdf: default  
params:
  wordcount: |
    <strong>{{< words-sum body-note-ref >}} total words</strong>: {{< words-body >}} in the body • {{< words-ref >}} in the #references • {{< words-note >}} in the notes
keywords:
  - bibliometrics
  - research evaluation 
  - OpenAlex
  - Web of Science
  - open data
  - metadata
execute:
  warning: false
  message: false
  error: false
  echo: false
editor_options: 
  chunk_output_type: inline
---

# Introduction

Bibliometrics has been used in research and data-driven research evaluation for approximately half a century [@narin1976evaluative], originally supported by the development of the Web of Science (WoS), which was conceptualized in 1955 and launched in 1964 [@clarivate_history]. It took more than 30 years for other players to emerge: Elsevier’s Scopus was founded in 1996, Crossref in 1999, Google Scholar in 2004, Microsoft Academic (now integrated into OpenAlex) and Dimensions in 2018, and OpenAlex in 2022[^1]. Myriad other databases exist to support specific bibliometric work or topical analysis; PubMed, for example, primarily indexes life sciences and biomedical works and The Lens provides a view into innovation through its coverage of patent knowledge[^2].

[^1]: <https://www.webofscience.com/wos/> (originally named the Science Citation Index); <https://www.scopus.com/>; <https://www.crossref.org/>;  <https://scholar.google.com/>;  <https://www.dimensions.ai/>;  <https://openalex.org/>.

[^2]: <https://pubmed.ncbi.nlm.nih.gov/>; <https://www.lens.org/>.

The bibliometrics field has long paid attention to the coverage and quality of these data sources and investigated differences in evaluative outcomes produced from them[^3]. @bruce2004continuum note that the library community’s efforts to define quality in bibliographic records are linked, in part, to their aim to enforce it. The advent of OpenAlex- an open database that indexes over 250 million scholarly works with broader coverage of the Humanities, non-English languages, and the Global South than traditional indexes [@priem2022openalex] generated a new wave of such studies. As an aggregator and standardizer of data from Microsoft Academic Graph (MAG), Crossref, Open Researcher and Contributor ID (ORCID), Research Organization Registry (ROR), Directory of Open Access Journals (DOAJ), Unpaywall, PubMed, and several other sources[^4], its coverage and quality have been of interest regarding its efficacy for quantitative analysis. This scrutiny has resulted in observations that OpenAlex might contain erroneous metadata, or that it is still largely incomplete. For example, changing processes for matching and disambiguating data have demonstrated the importance of persistent identifiers and remaining cautious when creating new open data sources dependent on OpenAlex [@mongeon2024twitter].

[^3]: Turgel and Chernova (2024) usefully point out that “citation”, “bibliometric”, and “Scientometric” databases are all distinct. Citation databases focus on citation connections between documents; similarly, bibliometric databases focus on publications and their citation metrics. Scientometric databases possess a range of metadata and indicators for more robust analyses of the structure and dynamics or the research landscape. They define Web of Science and OpenAlex as Scientometric databases (pg. 4).

[^4]: See OpenAlex’s data sources page: <https://help.openalex.org/hc/en-us/articles/24397285563671-About-the-data>

## Coverage of OpenAlex versus Web of Science

As a nascent data source, numerous recent studies have compared the coverage of OpenAlex to more established databases. Culbert et al. (2024) investigated the coverage of reference items between OpenAlex, WoS, and Scopus. They found that OpenAlex was comparable with commercial databases from an internal reference coverage perspective if restricted to a core corpus of publications similar to the other two sources, though it lacked cited references. Low reference, funder, and affiliation coverage was also found by @alonso2024coverage, though they observed OpenAlex’s coverage of publication and author information was high compared to WoS and Scopus. @simard2025 and @maddi2024coverage investigated the OA journal coverage of OpenAlex, WoS, and Scopus using the DOAJ and ROAD databases as reference databases. Both studies found that OpenAlex indexes more journals and provides more balanced geographical coverage. @cespedes2024linguistic determined that OpenAlex’s linguistic coverage (75% English) far surpassed that of WoS (95% English) from the metadata, with the former reduced to 68% upon manual verification of the works themselves. However, the language field in OpenAlex is algorithmically detected from the title and abstract metadata, introducing limitations[^5]. In a coverage comparison of six databases, @ortega2024retracted found that OpenAlex indexes more retracted works than WoS, Scopus, and PubMed.

[^5]: <https://docs.openalex.org/api-entities/works/work-object#language>

## Metadata quality of OpenAlex versus Web of Science

@donner2017document found document type (articles, reviews, letters, and others) discrepancies between WoS and journal websites or article full text for a sample of 791 publications, with letters and reviews particularly affected. Different average page and reference counts for errors were also observed for arising discrepancies, hinting at the potential development of semi-automatic methods using these findings.

Other studies focused on the quality of OpenAlex data and showed that institutional metadata is missing from many OpenAlex records [@bordignon2024openalex; @zhang2024institutions], and funding metadata is also lacking @schares2024funder. @haupka2024types observed a broader range of materials as research publications in OpenAlex compared to Scopus, WoS, and PubMed, potentially explained by @ortega2024retracted as resulting from the database’s reliance on Crossref’s less granular system of classification.

## Research objectives

The rise of open science has promoted the creation, sustainability, and use of open infrastructure; the completeness and accuracy of such sources are compared frequently to proprietary, curated databases which tout more exclusive standards for indexing, but make their data curation processes less transparent. However, in describing how the Citation Index Project (an early version of what is now the Web of Science) could be used for evaluation exercises, Eugene Garfield emphasized how such information could be used “as a basis for comparative studies on the efficacy of various indexing techniques” [@garfield1963citation, p. 195]. Indeed, studies of this kind thus serve as “vital statistics” which allude to major implications of bibliometric analyses conducted using different data sources. For example, @liu2021samejournal found major discrepancies between publication records for the same journal indexed in WoS and Scopus, respectively, forcing users into the dilemma of Segal’s law[^6]. Thus, this study aims to compare discrepancies between OpenAlex, a massive open bibliographic database increasingly utilized by the bibliometrics community since its launch, and WoS, an entrenched source of data regarded for its high-quality curation and standardization and recently found to be the most reliable in terms of data completeness and accuracy [@singh2024fields].

[^6]: Segal’s law states that “A man with a watch knows what time it is. A man with two watches is never sure” [@segal_law_wikipedia]

While OpenAlex’s coverage has been under recent intense investigation to evaluate its usability for accurate analyses, more needs to be understood about the comparability of its metadata to other data sources. Our work thus aims to assess the quality of metadata in OpenAlex and WoS. More specifically, it addresses the following research questions:

-   **RQ1**: How frequent are discrepancies between WoS and OpenAlex records in terms of document type, language, publication year, and number of authors?
-   **RQ3**: What share of records with discrepancies are correct in WoS, OpenAlex, neither, or both??
-   **RQ4**: What explains these discrepancies?

Scientific literature engages with several concepts in various combinations in relation to data sources, including *quality*, *accuracy*, *validity*, *completeness*, *reliability*, and *consistency*. Bruce and Hillman (2004) describe the most recognized characteristics of data quality as completeness, accuracy, provenance, conformance to expectations, logical consistency and coherence, timeliness, and accessibility[^7]. In this study, we largely refer to quality broadly, largely as accuracy and completeness, and we consider the relationship of both of these concepts in relation to their consistency. Thus, the examination of discrepancies reveals not only if and which errors are occurring, but also in what frequency. Findings regarding consistency may implicate the reliability of the data, potentially proving especially useful to those running large-scale analyses. In aggregate, this study’s findings may also point to possible reasons for such errors which may have useful implication for the validation of this data and avoidance of future error through the updating of database processes, considering that these errors are difficult to identify [@meester2016response].

[^7]: See @bruce2004continuum for a definition of each of these characteristics.

# Data and methods

## Data collection

The WoS data used in this study was retrieved from a relational database version of the WoS hosted by the Observatoire des sciences et des technologies (OST) and limited to the Science Citation Index (SCI), the Social Sciences Citation Index (SSCI), and the Arts & Humanities Citation Index (A&HCI). We collected all WoS records with a DOI published between 2021 and 2023 (N = 7,661,474). We removed 30,3094 (0.4%) WoS records with multiple document types to avoid complications with the analysis. Of the remaining 7,631,080 WoS records, 6,599,479 (86.5%) had a DOI match in the February 2024 snapshot of OpenAlex accessed through Google Big Query [see @mazoni2024]. We used the subset of 6,594,747 Web of Science records with a single DOI match in OpenAlex for our analysis. In the OST database, every journal is assigned to one of 143 specialties and 14 disciplines of the National Science Foundation (NSF) classification. For our analysis, we grouped the disciplines into four groups: Arts and Humanities (AH), Biomedical Research (BM), Natural Science and Engineering (NSE), and Social Sciences (SS).

## Identification of discrepancies

For each matching WoS and OpenAlex record, we compared the following four metadata elements: 1) document type, 2) language, 3) publication year, and 4) number of authors.

For the document type, we did not consider discrepancies where a record is a review according to WoS and an article according to OpenAlex, and vice versa (i.e., we consider articles and reviews as the same document type). Furthermore, OpenAlex indexes conference papers as articles, and the source type (conference) is meant to distinguish them from journal articles. For these reasons, we only considered discrepancies for which the record is identified as an article or a review in either WoS or OpenAlex, but is identified as neither in the other source. We also excluded discrepancies for which the record is a meeting abstract in WoS and an article in OpenAlex. Overall, we found 427,593 discrepancies that met these criteria (6.5% of all records in the dataset).

The identification of discrepancies in language, publication year, and number of authors was limited to the subset of 5,924,459 articles and reviews with no discrepancy in document type (i.e., the records that are articles or reviews in both WoS and OpenAlex). The publication language is recorded in its long form (e.g., English) in our WoS data and its ISO code (e.g., EN) in OpenAlex, so we created a list of all language combinations (e.g., English-EN, English-FR), determined which ones constitute discrepancies, and identified 33,322 discrepancies (0.6% of all articles and reviews) in publication language between WoS and OpenAlex. The publication year and number of authors being numeric indicators, identifying discrepancies in these metadata fields is easily done by calculating the difference between the two values and flagging all non-zero values. We identified a total of 469,098 discrepancies (7.9% of all articles and reviews) in publication years and 70,590 discrepancies (1.2% of all articles and reviews) in the number of authors. We recorded them both as dichotomous variables (discrepancy vs no discrepancy) and as differences between the two values, which allowed us to measure both the frequency and the strength of the discrepancies.

## Investigation of discrepancies

### Discrepancies Sample

For the publication year and number of authors, we also stratified using six bins based on the difference between the values in WoS and OpenAlex (-4 or less, -2 to -3, -1, 1, 2 to 3, 4 or more) to ensure that a range of cases are represented in the data.

### Investigation Process

The sample discrepancies were manually investigated by looking at the article on the journal’s website and the full text when necessary and available. We recorded whether the WoS or OpenAlex record was correct and, when possible, explained the discrepancy. For the publication year, it is typical for articles to have two publication dates: the date of the first online publication and the date of the publication of the issue. For cases in which the online publication year was recorded in one database and the issue year was recorded in the other, we did not consider either database to be correct.

For author discrepancies, the landing page and, when available, the published version was examined for author counts. Where available, author declarations, acknowledgements, and CRediT declarations were also considered to clarify authorship where groups or consortia were involved in the production of the work. For languages, the full text was the primary source, followed by the publisher's landing page for recording observations on language assignment. We recorded multiple language full texts if available, as well as translated abstracts available on the landing page. There are limitations to using the publisher landing page with a web browser.  The language encoding in the head of the HTML could be used by the GET request to display your preferred language type, if the server is configured to respond to such a request.

This process allowed us to go beyond the simple counting of differences between the databases and gain insight into the different factors that can cause discrepancies and estimate the percentage of erroneous records in each database. One caveat, of course, is that because our process relies on discrepancies in identifying errors, we are not considering cases where both databases contain the same error.

# Results

```{r eval = TRUE}
library(tidyverse)
library(flextable)
library(officer)
library(quarto)
library(psych)
library(viridis)
library(hrbrthemes)
library(ggsci)
```

```{r echo=FALSE, eval=FALSE, message=FALSE, warning=FALSE}
data<-readRDS("data/_data.rds")
article_data<-readRDS("data/_data_articles.rds")
```

We structured our results section by type of discrepancy. For each, we first show the distribution of values in the Web of Science and the OpenAlex datasets. Note that these are not the distributions in the entire Web of Science OpenAlex databases, but the distributions in the subset of records with a DOI match between the databases that are included in our analysis. Second, we present descriptive statistics related to the discrepancies identified and the sample that was used for investigations. Finally, we present the results of these investigations.

## Document type

Table 2 and Table 3 present the distribution of records across document types in WoS and OpenAlex, respectively, to provide a general picture of the databases’ content and the differences in their classification. While WoS contains twice as many document types as OpenAlex, these differences appear mainly among the less frequent types, in line with the findings of @haupka2024types. Most documents in both data sources are articles and reviews.

```{r echo=FALSE, eval=FALSE, message=FALSE, warning=FALSE}

table1 <- data %>% 
  group_by(wos_type) %>% 
  reframe(n = n()) %>% 
  arrange(desc(n)) %>% 
  rownames_to_column("rank") %>% 
  mutate(rank = as.numeric(rank)) %>% 
  mutate(wos_type = ifelse(rank < 11,wos_type,"other")) %>% 
  group_by(wos_type) %>% 
  reframe(n = sum(n)) %>% 
  mutate(pct = round(n / sum(n) * 100,2)) %>% 
  arrange(desc(n))

table1 <- bind_rows(table1, summarise(table1, wos_type = "total", n = sum(n), pct=100))

table1 %>% 
  writexl::write_xlsx("data/table1.xlsx")

```

```{r echo=FALSE, eval=FALSE, message=FALSE, warning=FALSE}
library(flextable)
library(officer)

table1 <- readxl::read_xlsx("data/table1.xlsx") %>% 
  rename(`WoS Document Type` = wos_type)
  
ft <- table1 %>%
  flextable() %>%
  set_table_properties(width = 1, layout = "autofit") %>%
  border(i = nrow(table1), border.top = fp_border(width = 1)) %>%
  padding(i = NULL, j = NULL,
          padding.top = 2,
          padding.bottom = 2,
          padding.left = 2,
          padding.right = 2) %>%
  align(j = "n", align = "right", part = "all") %>%
  set_caption("Table 1. Distribution of records by document type in WoS", align_with_table = FALSE)

# ft <- footnote(
#   x = ft,
#   notes = list(
#     body = list(
#       i = 11,
#       j = "WoS Document Type",
#       value = as_paragraph(
#         "Includes cc meeting heading, poetry, expression of concern, reprint, art exhibit review, item withdrawal, film review, bibliography, fiction, creative prose, theater review, record review, music performance review, software review, music score review, hardware review, tv review, radio review, dance performance review, excerpt, database review, data paper, note, and script. It should be noted that TV Review, Radio Review, Video Review were retired as document types and are no longer added to items indexed in the WoS Core Collection. They are still usable for searching or refining/analyzing search results."
#       ),
#       ref_symbols = "*"
#     )
#   )
# )

ft
```

```{r echo=FALSE, eval=FALSE, message=FALSE, warning=FALSE}

table2 <- data %>% 
  group_by(oa_type) %>% 
  reframe(n = n()) %>% 
  arrange(desc(n)) %>% 
  rownames_to_column("rank") %>% 
  mutate(rank = as.numeric(rank)) %>% 
  mutate(oa_type = ifelse(rank < 11,oa_type,"other")) %>% 
  group_by(oa_type) %>% 
  reframe(n = sum(n)) %>% 
  mutate(pct = round(n / sum(n) * 100,2)) %>% 
  arrange(desc(n))

table2 <- bind_rows(table2, summarise(table2, oa_type = "total", n = sum(n), pct=100))

writexl::write_xlsx(table2, "data/table2.xlsx")
```

```{r}
table2<- readxl::read_xlsx("data/table2.xlsx")
ft<-table2 %>% 
  flextable() %>% 
  set_table_properties(width = 1, layout = "autofit") %>%   border(i = nrow(table2) , border.top = fp_border(width = 1)) %>% 
  padding(i = NULL, j = NULL, 
          padding.top = 2,
          padding.bottom = 2,
          padding.left = 2,
          padding.right = 2) %>% 
  align(j = 2:3, align = "right", part = "all")  %>% 
  set_caption("Table 2. Distribution of records by document type in OpenAlex", align_with_table = FALSE) 

# ft<-ft %>% 
#   footnote(i = 11, j = 1, value = as_paragraph("Includes report, dataset, other, dissertation, supplementary-materials, and reference-entry."),ref_symbols = "*", part = "body") %>% 

ft
```

Tables 4 and 5 show that the vast majority 301,850 of the 310,843 discrepancies (97%) are cases where a record is an article or review in OpenAlex but not WoS. We find that in both tables the majority of discrepancies indicate a misclassification of a record as an article or review. However, WoS is much more accurate at detecting misclassification in OpenAlex (93.5% of the sample) than in WoS (68.8% of the sample). This generally points to a much larger number of OpenAlex records misclassified as articles or review compared to WoS.

```{r echo=FALSE, eval=FALSE, message=FALSE, warning=FALSE}
a<-readxl::read_xlsx("data/_all_doc_type_discrepancies.xlsx")
sample<- readxl::read_xlsx("data/_final_wos_doc_type_sample.xlsx")

x<-a %>% 
  filter(wos_type %in% c("article","review")) %>% 
  left_join(sample %>% select(doi, most_accurate_source, note, correct_type), 
            by="doi") %>% 
  mutate(oa_type = ifelse(oa_type %in% c("report", "dataset", "other", "dissertation" , "supplementary-materials","reference-entry"), "other", oa_type)) %>%
  group_by(oa_type) %>% 
  reframe(Discrepancies_n = n()) %>% 
  mutate(Discrepancies_pct = round(Discrepancies_n/sum(Discrepancies_n)*100,1)) %>% 
  arrange(desc(Discrepancies_n))

y<-a %>% 
  filter(wos_type %in% c("article","review")) %>% 
  inner_join(sample %>% select(doi, most_accurate_source, note, correct_type), 
            by="doi") %>% 
  mutate(oa_type = ifelse(oa_type %in% c("report", "dataset", "other", "dissertation" , "supplementary-materials","reference-entry"), "other", oa_type)) %>%
  group_by(oa_type) %>% 
  reframe(Sample_n = n()) %>% 
  mutate(Sample_pct = round(Sample_n/sum(Sample_n)*100,1))

z<-a %>%  
  filter(wos_type %in% c("article","review")) %>% 
  inner_join(sample %>% select(doi, most_accurate_source, note, correct_type), 
            by="doi") %>% 
  mutate(oa_type = ifelse(oa_type %in% c("report", "dataset", "other", "dissertation" , "supplementary-materials","reference-entry"), "other", oa_type)) %>%
  filter(most_accurate_source != "wos") %>% 
  group_by(oa_type) %>% 
  reframe(Errors_n = n())

table3<- x %>% 
  left_join(y, by="oa_type") %>% 
  left_join(z, by="oa_type") %>% 
  mutate(Errors_pct = round(Errors_n/Sample_n*100,1)) %>% 
  rename(`Type in OpenAlex` = oa_type) %>% 
  mutate(across(everything(), ~replace_na(.x, 0)))

  

table3 <- bind_rows(table3, 
                    summarise(table3, `Type in OpenAlex` = "total", 
                              Discrepancies_n = sum(Discrepancies_n),
                              Discrepancies_pct=100,
                              Sample_n = sum(Sample_n),
                              Sample_pct = 100,
                              Errors_n = sum(Errors_n)) %>% 
                    mutate(Errors_pct = ifelse(`Type in OpenAlex` == "total",
                                               round(sum(Errors_n)/sum(Sample_n)*100,
                                                     1),
                                               Errors_pct)))
writexl::write_xlsx(table3, "data/table3.xlsx")
```

```{r echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
table3<- readxl::read_xlsx("data/table3.xlsx")

table3 %>% 
  flextable() %>% 
  set_table_properties(width = 1, layout = "autofit") %>%   
  border(i = nrow(table3), border.top = fp_border(width = 1)) %>% 
  set_caption("Table 3. Discrepancies for documents categorized as articles or reviews in the WoS but not in OpenAlex", align_with_table = FALSE) %>% 
  padding(i = NULL, j = NULL, 
          padding.top = 2,
          padding.bottom = 2,
          padding.left = 2,
          padding.right = 2) %>% 
  separate_header() %>% 
  align(j = 2:6, align = "right", part = "all") %>% 
  width(j = 1, width = 1.5)
```

```{r echo=FALSE, eval=FALSE, message=FALSE, warning=FALSE}
a<-readxl::read_xlsx("data/_all_doc_type_discrepancies.xlsx") %>% 
  filter(wos_type != "meeting abstract")

sample<- readxl::read_xlsx("data/_final_oa_doc_type_sample.xlsx")

x<-a %>% 
  filter(oa_type %in% c("article","review")) %>% 
  left_join(sample %>% select(doi, most_accurate_source, note), 
            by="doi") %>% 
  mutate(wos_type = ifelse(wos_type %in% c("editorial material","book review","letter", "correction", "news item","biographical-item","retraction"), wos_type, "other")) %>%
  group_by(wos_type) %>% 
  reframe(Discrepancies_n = n()) %>% 
  mutate(Discrepancies_pct = round(Discrepancies_n/sum(Discrepancies_n)*100,1)) %>% 
  arrange(desc(Discrepancies_n))

y<-a %>% 
  filter(oa_type %in% c("article","review")) %>% 
  inner_join(sample %>% select(doi, most_accurate_source, note), 
            by="doi") %>% 
  mutate(wos_type = ifelse(wos_type %in% c("editorial material","book review","letter", "correction", "news item","biographical-item","retraction"), wos_type, "other")) %>%
  group_by(wos_type) %>% 
  reframe(Sample_n = n()) %>% 
  mutate(Sample_pct = round(Sample_n/sum(Sample_n)*100,1))

z<-a %>%  
  filter(oa_type %in% c("article","review")) %>% 
  inner_join(sample %>% select(doi, most_accurate_source, note), 
            by="doi") %>% 
    mutate(wos_type = ifelse(wos_type %in% c("editorial material","book review","letter", "correction", "news item","biographical-item","retraction"), wos_type, "other")) %>% 
  filter(most_accurate_source != "oa") %>% 
  group_by(wos_type) %>% 
  reframe(Errors_n = n())

table4<- x %>% 
  left_join(y, by="wos_type") %>% 
  left_join(z, by="wos_type") %>% 
  mutate(Errors_pct = round(Errors_n/Sample_n*100,1)) %>% 
  rename(`Type in WoS` = wos_type) %>% 
  mutate(across(everything(), ~replace_na(.x, 0)))

  

table4 <- bind_rows(table4, 
                    summarise(table4, `Type in WoS` = "total", 
                              Discrepancies_n = sum(Discrepancies_n),
                              Discrepancies_pct=100,
                              Sample_n = sum(Sample_n),
                              Sample_pct = 100,
                              Errors_n = sum(Errors_n)) %>% 
                    mutate(Errors_pct = ifelse(`Type in WoS` == "total",
                                               round(sum(Errors_n)/sum(Sample_n)*100,
                                                     1),
                                               Errors_pct)))

writexl::write_xlsx(table4, "data/table4.xlsx")
```

```{r echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
table4<-readxl::read_xlsx("data/table4.xlsx") 
table4 %>% 
  flextable() %>% 
  set_table_properties(width = 1, layout = "autofit") %>%   
  border(i = nrow(table4), border.top = fp_border(width = 1)) %>% 
  set_caption("Table 4. Discrepancies for documents categorized as articles or reviews in OpenAlex but not in WoS", align_with_table = FALSE) %>% 
  padding(i = NULL, j = NULL, 
          padding.top = 2,
          padding.bottom = 2,
          padding.left = 2,
          padding.right = 2) %>% 
  separate_header() %>% 
  align(j = 2:7, align = "right", part = "all") %>% 
  width(j = 1, width = 1.5) 

```

## Publication Language

### Distribution of Records by Language in WoS and OpenAlex

Table 5 and Table 6 show the distribution of articles and reviews across languages in WoS and OpenAlex, respectively, limited to the top 10 most frequent languages. Unsurprisingly, the quasi totality of records are in English in both databases. It is important to note, once again, that our results are note meant to reflect the compositon of the entire databases, but only the subset of documents included in both, which expectedly underestimate the representation of non-English literature especially on the OpenAlex side. Also important to reiterate here is that we our WoS data includes only the SCI-E, SSCI, and A&HCI, and not the ESCI which would include more non-English records.

```{r eval=FALSE}
table5 <- data_articles %>% 
  group_by(wos_language) %>% 
  reframe(n = n()) %>% 
  arrange(desc(n)) %>% 
  rownames_to_column("rank") %>% 
  mutate(rank = as.numeric(rank)) %>% 
  mutate(wos_language = ifelse(rank < 11,wos_language,"Other")) %>% 
  group_by(wos_language) %>% 
  reframe(n = sum(n)) %>% 
  mutate(pct = round(n / sum(n) * 100,2)) %>% 
  arrange(desc(n))

table5 <- bind_rows(table5, summarise(table5, 
                                      wos_language = "Total", 
                                      n = sum(n), 
                                      pct=100))

table5 %>% 
  rename(`Language in WoS` = wos_language,
         Records_n = n,
         Records_pct = pct) %>% 
  writexl::write_xlsx("data/table5.xlsx")

```

```{r eval=TRUE}

table5<-readxl::read_xlsx("data/table5.xlsx") %>% 
  slice(c(1:5,7:11,6,12))

table5 %>% 
  flextable() %>% 
  set_table_properties(width = 1, layout = "autofit") %>%   
  border(i = nrow(table5), 
         border.top = fp_border(width = 1)) %>%
  padding(i = NULL, j = NULL, 
          padding.top = 2,
          padding.bottom = 2,
          padding.left = 2,
          padding.right = 2) %>% 
  separate_header() %>% 
  align(j = 2:3, align = "right", part = "all") %>% 
  width(j = 1, width = 1.5)  %>% 
  set_caption("Table 5. Distribution of records by language in WoS", align_with_table = FALSE)
  
```

Aside from the dominance of English in the dataset, we also observe differences in the languages that compose the top 10 in each database, as well as in their ranking. Spanish, French, and Portugese are much more prevalent in the OpenAlex records than in the WoS. On the other hand, Chinese is the 5th most prevalent language in the WoS data, but does not make the top 10 in OpenAlex. Since these are supposed to be the same set of records, these differences point to significant discrepancies in the language between the two databases.

```{r eval = FALSE}

table6 <- data_articles %>% 
  group_by(oa_language) %>% 
  reframe(n = n()) %>% 
  arrange(desc(n)) %>% 
  rownames_to_column("rank") %>% 
  mutate(rank = as.numeric(rank)) %>% 
  mutate(oa_language = ifelse(rank < 12,oa_language,"Other")) %>% 
  group_by(oa_language) %>% 
  reframe(n = sum(n)) %>% 
  mutate(pct = round(n / sum(n) * 100,2)) %>% 
  arrange(desc(n))

table6 <- bind_rows(table6, summarise(table6, 
                                      oa_language = "Total", 
                                      n = sum(n), 
                                      pct=100))

table6 %>% 
  rename(`Language in OpenAlex` = oa_language,
         Records_n = n,
         Records_pct = pct) %>% 
  writexl::write_xlsx("data/table6.xlsx")
```

```{r eval = TRUE}

table6<-readxl::read_xlsx("data/table6.xlsx") %>%
  slice(c(1:4,6,8:12,5,7,13)) %>% 
  mutate(`Language in OpenAlex` = case_when(`Language in OpenAlex` == "en" ~ "English",
                                 `Language in OpenAlex` == "es" ~ "Spanish",
                                 `Language in OpenAlex` == "de" ~ "German",
                                 `Language in OpenAlex` == "fr" ~ "French",
                                 `Language in OpenAlex` == "pt" ~ "Portugese",
                                 `Language in OpenAlex` == "pl" ~ "Polish",
                                 `Language in OpenAlex` == "tr" ~ "Turkish",
                                 `Language in OpenAlex` == "hu" ~ "Hungarian",
                                 `Language in OpenAlex` == "hr" ~ "Croatian",
                                 `Language in OpenAlex` == "ro" ~ "Romanian",
                                 is.na(`Language in OpenAlex`) ~ "Unknown",
                                 `Language in OpenAlex` == "Total" ~ "Total",
                                 `Language in OpenAlex` == "Other" ~ "Other"
  )) 
  
table6 %>% 
  flextable() %>% 
  set_table_properties(width = 1, layout = "autofit") %>%  
  border(i = nrow(table6), border.top = fp_border(width = 1)) %>% 
  padding(i = NULL, j = NULL, 
          padding.top = 2,
          padding.bottom = 2,
          padding.left = 2,
          padding.right = 2) %>% 
  separate_header() %>% 
  align(j = 2:3, align = "right", part = "all") %>% 
  set_caption("Table 6. Distribution of records by language in OpenAlex", align_with_table = FALSE)
```

### Language discrepancies between WoS and OpenAlex

For this part of the analysis, we focus on articles and review that are in English in Web of Science or in OpenAlex, and in a language other than English in the other database. In other words, we do not investigate discrepancies between Portuguese and Spanish or French and German, but only discrepancies for which one of the languages is English (e.g., English-German, English-Chinese).

Table 7 and Table 8 respectively display the number of discrepancies found for articles in English in WoS and articles in English in OpenAlex. They also display the number of records manually investigated to determine whether the discrepancies stem from mislabeling non-English content as English, as well as the result of that investigation. To understand how an incorrect assignment may have occurred, we looked at the abstracts and full text on the landing page as well as the full text when it was available. Most sampled works seem to have multi-language abstracts either on the landing page or in the PDF of the full text. Also, some articles are published in multiple language, meaning that for some of the discrepancies observed, both OpenAelx and WoS are correct.

A first observation is that WoS appears to be more accurate than OpenALex in terms of publication language. Table Only 20,086 English publications in the WoS were in another language according to OpenAlex, and in slightly less than half (48.2%) of the verified cases, the article was in fact in English.

```{r eval = FALSE}


library(readxl)
library(tidyverse)
library(flextable)
library(officer)

a<-readxl::read_xlsx("data/_discrepancies_language.xlsx") %>% 
      mutate(oa_language = case_when(oa_language == "en" ~ "English",
                                 oa_language == "es" ~ "Spanish",
                                 oa_language == "de" ~ "German",
                                 oa_language == "fr" ~ "French",
                                 oa_language == "pt" ~ "Portugese",
                                 oa_language == "pl" ~ "Polish",
                                 oa_language == "tr" ~ "Turkish",
                                 oa_language == "hu" ~ "Hungarian",
                                 oa_language == "hr" ~ "Croatian",
                                 oa_language == "ro" ~ "Romanian",
                                 TRUE ~ "Other"))

sample<- readxl::read_xlsx("data/_final_wos_lang_sample.xlsx") %>% 
  filter(`who is correct?` %in% c("WoS is correct","OA is correct","Both are correct")) %>% 
    mutate(oa_language = case_when(oa_language == "en" ~ "English",
                                 oa_language == "es" ~ "Spanish",
                                 oa_language == "de" ~ "German",
                                 oa_language == "fr" ~ "French",
                                 oa_language == "pt" ~ "Portugese",
                                 oa_language == "pl" ~ "Polish",
                                 oa_language == "tr" ~ "Turkish",
                                 oa_language == "hu" ~ "Hungarian",
                                 oa_language == "hr" ~ "Croatian",
                                 oa_language == "ro" ~ "Romanian",
                                 TRUE ~ "Other"))

x<-a %>% 
  filter(wos_language == "English") %>% 
  group_by(oa_language) %>% 
  reframe(Discrepancies_n = n()) %>% 
  mutate(Discrepancies_pct = round(Discrepancies_n/sum(Discrepancies_n)*100,1)) %>% 
  arrange(desc(Discrepancies_n)) 

y<-sample %>% 
  filter(wos_language == "English") %>% 
  left_join(sample %>% select(2,6,7,8), 
            by="doi") %>%  
  group_by(oa_language) %>% 
  reframe(Sample_n = n()) %>% 
  mutate(Sample_pct = round(Sample_n/sum(Sample_n)*100,1))

z<-sample %>%  
  filter(wos_language == "English") %>% 
  filter(`who is correct?` != "OA is correct") %>% 
  group_by(oa_language) %>% 
  reframe(Errors_n = n())

table7<- x %>% 
  left_join(y, by="oa_language") %>% 
  left_join(z, by="oa_language") %>% 
  mutate(Errors_pct = round(Errors_n/Sample_n*100,1)) %>% 
  mutate(across(everything(), ~replace_na(.x, 0)))

  

table7 <- bind_rows(table7, 
                    summarise(table7, oa_language= "Total", 
                              Discrepancies_n = sum(Discrepancies_n),
                              Discrepancies_pct=100,
                              Sample_n = sum(Sample_n),
                              Sample_pct = 100,
                              Errors_n = sum(Errors_n)) %>% 
                    mutate(Errors_pct = ifelse(oa_language == "Total",
                                               round(sum(Errors_n)/sum(Sample_n)*100,
                                                     1),
                                               Errors_pct)))

writexl::write_xlsx(table7,"data/table7.xlsx")
```

```{r}

table7<-readxl::read_xlsx("data/table7.xlsx") %>%
  slice(c(1:3,5:10,4,11)) %>% 
  rename(`Language in OpenAlex` = oa_language)

table7 %>% 
  flextable() %>%
  set_table_properties(width = 1, layout = "autofit") %>%   
  border(i = nrow(table7), border.top = fp_border(width = 1)) %>% 
  padding(i = NULL, j = NULL, 
          padding.top = 2,
          padding.bottom = 2,
          padding.left = 2,
          padding.right = 2) %>% 
  separate_header() %>% 
  align(j = 2:6, align = "right", part = "all") %>% 
  width(j = 1, width = 1.5) %>% 
  set_caption("Table 7.Language discrepancies for articles and reviews in English in WoS", align_with_table = FALSE)
```

However, we notice that for some languages, such as French, German, Romanian and Croatian, the discrepancies correctly flagged articles mislabelled as English in WoS., although the sample sizes for Romanian and Croatian were quite small. On the other hand, the majority of articles labelled as English in WoS and as Spanish, Portuguese, or Turkish in OpenAlex were actually found to be in English.

```{r eval = FALSE}
 a<-readxl::read_xlsx("data/_discrepancies_language.xlsx")
 sample<- readxl::read_xlsx("data/_final_oa_lang_sample.xlsx")
   filter(`who is correct?` %in% c("WoS is correct","OA is correct","Both are correct"))

x<-a %>% 
   filter(oa_language == "en") %>% 
   group_by(wos_language) %>% 
   reframe(Discrepancies_n = n()) %>% 
   mutate(Discrepancies_pct = round(Discrepancies_n/sum(Discrepancies_n)*100,1)) %>% 
   arrange(desc(Discrepancies_n)) 
 
y<-sample %>% 
   filter(oa_language == "en") %>% 
   group_by(wos_language) %>% 
   reframe(Sample_n = n()) %>% 
   mutate(Sample_pct = round(Sample_n/sum(Sample_n)*100,1))
 
sample %>% 
   filter(oa_language == "en")
   group_by(`who is correct?`) %>% 
   reframe(n = n())
 
z<-sample %>%  
   filter(`who is correct?` != "OA is correct") %>% 
   group_by(oa_language) %>% 
   reframe(Errors_n = n())
 
table7<- x %>% 
#   left_join(y, by="oa_language") %>% 
#   left_join(z, by="oa_language") %>% 
#   mutate(Errors_pct = round(Errors_n/Sample_n*100,1)) %>% 
#   rename(`Language in OpenAlex` = oa_language) %>% 
#   mutate(across(everything(), ~replace_na(.x, 0)))
# 
#   
# 
# table7 <- bind_rows(table7, 
#                     summarise(table7, `Language in OpenAlex` = "total", 
#                               Discrepancies_n = sum(Discrepancies_n),
#                               Discrepancies_pct=100,
#                               Sample_n = sum(Sample_n),
#                               Sample_pct = 100,
#                               Errors_n = sum(Errors_n)) %>% 
#                     mutate(Errors_pct = ifelse(`Language in OpenAlex` == "total",
#                                                round(sum(Errors_n)/sum(Sample_n)*100,
#                                                      1),
#                                                Errors_pct)))
# 
# table7 %>% 
#   flextable() %>% 
#   set_table_properties(width = 1, layout = "autofit") %>%   border(i = 12, border.top = fp_border(width = 1)) %>% 
#   padding(i = NULL, j = NULL, 
#           padding.top = 2,
#           padding.bottom = 2,
#           padding.left = 2,
#           padding.right = 2) %>% 
#   separate_header() %>% 
#   align(j = 2:6, align = "right", part = "all") %>% 
#   width(j = 1, width = 1.5)
```

## Publication year

### Distribution of publication years in WoS and OpenALex

Now turning our attention to discrepancies in publication years for articles in WoS and OpenAlex, we first present a statistical summary of the

```{r eval=FALSE}
article_data %>% 
  select(discipline_group, value = oa_pub_year) %>% 
  mutate(database = "OpenAlex") %>% 
  bind_rows(article_data %>% 
              select(discipline_group, value = wos_pub_year) %>% 
              mutate(database = "WoS")) %>% 
  group_by(database) %>% 
  reframe(n = n(),
            mean = mean(value),
            sd = round(sd(value),3),
            var = round(var(value),3),
            q1 = quantile(value,0.25),
            median = median(value),
            q3 = quantile(value,0.75),
            min = min(value),
            max = max(value),
            skew = round(skew(value),3),
            kurtosis = round(kurtosi(value),3)
            ) %>% 
  writexl::write_xlsx("data/table9.xlsx")

# data_articles %>%
#   select(discipline_group, value = oa_pub_year) %>%
#   mutate(database = "OpenAlex") %>%
#   bind_rows(data_articles %>%
#               select(discipline_group, value = wos_pub_year) %>%
#               mutate(database = "WoS")) %>%
#   group_by(database) %>%
#   ggplot() +
#   aes(x=as.character(database), y = value) +
#   geom_boxplot()

```

```{r eval=TRUE}
table9<-readxl::read_xlsx("data/table9.xlsx") 

table9 %>% 
  rename(Database = database) %>% 
  flextable() %>% 
  set_table_properties(width = 1, layout = "autofit") %>%   
  border(i = nrow(table9), border.top = fp_border(width = 1)) %>% 
  padding(i = NULL, j = NULL, 
          padding.top = 2,
          padding.bottom = 2,
          padding.left = 2,
          padding.right = 2) %>% 
  separate_header() %>% 
  bold(part = "header") %>%
  align(j = 2:11, align = "right", part = "all") %>% 
  set_caption("Table 9. descriptive statistics summary of publication year in for articles and reviews in OpenAlex and WoS", align_with_table = FALSE)
```

### Publication Year Discrepancies Between WoS and OpenAlex

We observe a high prevalence of publication year discrepancies in articles and reviews in the WoS and OpenAlex, with 470,107 cases identified (see Table 10). We grouped the cases bases on the the direction and extent of the discrepancies by substracting the OpenAlex publication year from the WoS publication year. A difference of 1 indicates that according to WoS the article was published one year later than accoridng to OpenAlex. Inversely, a difference of -1 indicates that the articles was published one year ealier accoring to WoS than according to OpenAlex.

```{r eval = FALSE}
a<-readxl::read_xlsx("data/_discrepancies_pub_year.xlsx") %>% 
  select(doi, oa_pub_year, wos_pub_year) %>% 
  mutate(diff_pub_year = wos_pub_year - oa_pub_year) %>% 
  filter(diff_pub_year != 0) %>%
  mutate(bin = case_when(diff_pub_year < -3 ~ "< -3",
                         diff_pub_year %in% (-2:-3) ~ "-2 to -3",
                         diff_pub_year == -1 ~ "-1",
                         diff_pub_year == 1 ~ "1",
                         diff_pub_year %in% 2:3 ~ "2 to 3",
                         diff_pub_year > 3 ~ "> 3",
                         )) %>%
  group_by(bin) %>% 
  reframe(n = n())

a<-a %>% 
  mutate(pct = round(n/sum(n)*100,1)) %>%
  slice(c(2,1,3:5)) %>% 
  add_row(bin = "Total", n = sum(a$n), pct = 100)

sample<- readxl::read_xlsx("data/_final_pub_year_sample.xlsx") %>% 
    mutate(bin = case_when(diff_pub_year < -3 ~ "< -3",
                         diff_pub_year %in% (-2:-3) ~ "-2 to -3",
                         diff_pub_year == -1 ~ "-1",
                         diff_pub_year == 1 ~ "1",
                         diff_pub_year %in% 2:3 ~ "2 to 3",
                         diff_pub_year > 3 ~ "> 3",
                         )) %>% 
  group_by(bin) %>% 
  reframe(Sample_n = n()) %>%
  mutate(Sample_pct = round(Sample_n/sum(Sample_n)*100,1))

sample<-sample %>% 
    add_row(bin = "Total", Sample_n = sum(sample$Sample_n), Sample_pct = 100) 

a %>% 
  left_join(sample, by="bin") %>% 
  rename(Discrepancies_n = n, Discrepancies_pct = pct) %>% 
  writexl::write_xlsx("data/table10.xlsx")
```

```{r echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE}
table10 <- readxl::read_xlsx("data/table10.xlsx") %>% 
    rename(`Difference in publication year (WoS - OpenAlex)` = bin)


table10 %>% 
  flextable() %>% 
  set_table_properties(width = 1, layout = "autofit") %>%   border(i = nrow(table10), border.top = fp_border(width = 1)) %>% 
  padding(i = NULL, j = NULL, 
          padding.top = 2,
          padding.bottom = 2,
          padding.left = 2,
          padding.right = 2) %>% 
  separate_header() %>% 
  bold(part = "header") %>%
  align(j = 2:5, align = "right", part = "all") %>% 
  set_caption("Table 10. Number of publication year discrepancies for articles and reviews in WoS and OpenAlex (total and sampled)", align_with_table = FALSE)
```

### Explanation for publication year discrepancies

Publication year was divided between ‘first published year’ and ‘issue year’ as both dates do not have to be the same. In the majority of works, OpenAlex correctly identified the first publication year and WoS assigned the issue year (476 works), followed by the opposite condition where WoS used the first published year and OpenAlex used the issue year (155 works). The remainder of the table includes explanations that identify where sources were unclear, not included, do not match, or were incorrect, or neither date was available. 255 works did not have a publication date, and 11 did not have the issue date.

```{r eval= TRUE, warning=FALSE, message=FALSE}

fig1<-readxl::read_xlsx("data/_final_pub_year_sample.xlsx") %>% 
  mutate(diff_pub_year = wos_pub_year - oa_pub_year) %>% 
  mutate(bin = case_when(diff_pub_year < -3 ~ "< -3",
                         diff_pub_year %in% (-2:-3) ~ "-2 to -3",
                         diff_pub_year == -1 ~ "-1",
                         diff_pub_year == 1 ~ "1",
                         diff_pub_year %in% 2:3 ~ "2 to 3",
                         diff_pub_year > 3 ~ "> 3",
                         )) %>% 
  group_by(bin, Code) %>%
  reframe(n = n())

fig1 %>% 
  ggplot() +
  aes(fill=Code, y=n, x=bin) + 
    geom_bar(position="fill", stat="identity") +
    scale_fill_npg() +
    theme_minimal() +
    xlab("") +
    labs(caption = "Figure 1. blabla") +
    theme(plot.caption = element_text(hjust = 0.5, size = 10))
```

## Number of authors

```{r eval = FALSE}
# a<-readxl::read_xlsx("data/_discrepancies_n_authors.xlsx") %>% 
#   select(doi, oa_n_author, wos_n_author, diff_n_author) %>% 
#   mutate(bin = case_when(diff_n_author < -3 ~ "< -3",
#                          diff_n_author %in% (-2:-3) ~ "-2 to -3",
#                          diff_n_author == -1 ~ "-1",
#                          diff_n_author == 1 ~ "1",
#                          diff_n_author %in% 2:3 ~ "2 to 3",
#                          diff_n_author > 3 ~ "> 3",
#                          )) %>%
#   group_by(bin) %>% 
#   reframe(n = n())
# 
# a<-a %>% 
#   mutate(pct = round(n/sum(n)*100,1)) %>%
#   slice(c(5,2,1,3,4,6)) %>% 
#   add_row(bin = "Total", n = sum(a$n), pct = 1)
# 
# sample<- readxl::read_xlsx("data/_final_n_author_sample.xlsx") %>%
#     select(doi, wos_n_author, oa_n_author, diff) %>% 
#     rename(diff_n_author = diff) %>% 
#     mutate(bin = case_when(diff_n_author < -3 ~ "< -3",
#                          diff_n_author %in% (-2:-3) ~ "-2 to -3",
#                          diff_n_author == -1 ~ "-1",
#                          diff_n_author == 1 ~ "1",
#                          diff_n_author %in% 2:3 ~ "2 to 3",
#                          diff_n_author > 3 ~ "> 3",
#                          )) %>% 
#   group_by(bin) %>% 
#   reframe(Sample_n = n()) %>%
#   mutate(Sample_pct = round(Sample_n/sum(Sample_n)*100,1))
# 
# sample<-sample %>% 
#     add_row(bin = "Total", Sample_n = sum(sample$Sample_n), Sample_pct = 1) 
# 
# a %>% 
#   left_join(sample, by="bin") %>% 
#   rename(Discrepancies_n = n, Discrepancies_pct = pct) %>% 
#   writexl::write_xlsx("data/table9.xlsx")

```

```{r echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE}
# table9 <- readxl::read_xlsx("data/table9.xlsx")
# 
# table9 %>% 
#   rename(`Difference in number of authors (WoS - OpenAlex)` = bin) %>% 
#   flextable() %>% 
#   set_table_properties(width = 1, layout = "autofit") %>%   border(i = nrow(table9), border.top = fp_border(width = 1)) %>% 
#   padding(i = NULL, j = NULL, 
#           padding.top = 2,
#           padding.bottom = 2,
#           padding.left = 2,
#           padding.right = 2) %>% 
#   separate_header() %>% 
#   align(j = 2:5, align = "right", part = "all")
#   #width(j = 1, width = 1.2)
```

```{r eval= TRUE, warning=FALSE, message=FALSE}
# library(viridis)
# library(hrbrthemes)
# library(ggsci)
# readxl::read_xlsx("data/_final_pub_year_sample.xlsx") %>% 
#   mutate(diff_pub_year = wos_pub_year - oa_pub_year) %>% 
#   mutate(bin = case_when(diff_pub_year < -3 ~ "< -3",
#                          diff_pub_year %in% (-2:-3) ~ "-2 to -3",
#                          diff_pub_year == -1 ~ "-1",
#                          diff_pub_year == 1 ~ "1",
#                          diff_pub_year %in% 2:3 ~ "2 to 3",
#                          diff_pub_year > 3 ~ "> 3",
#                          )) %>% 
#   group_by(bin, Code) %>%
#   reframe(n = n()) %>% 
#   ggplot() +
#   aes(fill=Code, y=n, x=bin) + 
#     geom_bar(position="fill", stat="identity") +
#     scale_fill_npg() +
#     theme_minimal() +
#     xlab("") +
#     labs(caption = "Figure 1. blabla") +
#     theme(plot.caption = element_text(hjust = 0.5, size = 10))
```

Author discrepancies can be ambiguous to determine a ‘correct’ number. Authorships were manually counted on the landing page, and where available, the full text, which we considered the authoritative number, given that the authors would have reviewed and approved it. Additionally, Crossref metadata for authors was also considered as a possible explanation, particularly as OpenAlex draws heavily from this data source. The following table contains explanations determined by the investigators as to whether the OpenAlex or WoS count was correct, and if not, why the information might not be available. In rare cases, some articles contained over 1000 authors and were set aside for the time as being ‘too many to count’ manually.

In Table 6, the majority were found to have OpenAlex as the correct author reporting at 594 counts out of 1107 (53.7%), and WoS author counts were correct 368 (33.2%). Neither was found to be correct 91 times (8.2%). Bad URLs (meaning they are non-resolving, behind a paywall with no landing page, or were links to XML pages meant for similarity checking, among other possibilities) were found 2.3% of the time, followed by the lack of landing pages for works 1.4% of the time. Only 2 (0.2%) were found to be unregistered at the time, and only 1 (0.1%) was shown as ‘withdrawn’ by the publisher. In Table 7, author count explanations are examined regarding disciplines.

There are no large discrepancies biased towards one discipline or the other. Social Sciences had slightly more correct authors assigned by OpenAlex than the other disciplines, whereas BM had slightly more attributions by WoS as correct. BM also had more where neither was correct, followed by NSE, which is to be expected as both of these disciplines typically have more consortia, research groups, or teams attributed as authors, which can be tricky to determine if not explicitly declared by the authors. AH and NSE had more bad URLs, and AH contained the majority of those with no landing page for the work, making it beyond the scope of our methods. NSE had the majority of those over 1000 authors, which is not surprising given the large teams in fields such as Physics.

# Discussion

The value of bibliographic data sources is derived from different elements, including their coverage, completeness, and data accuracy (Visser et al., 2021). This research contributes to understanding the value and utility of OpenAlex as a data source by investigating its metadata discrepancies concerning document type. Our findings support those of past studies that have found metadata quality in OpenAlex to be in need of improvement lower in comparison to WoS. As Haupka et al. (2024) have indicated, document type classifications are critical for bibliometric research and evaluation, and so improved accuracy of document types is desired. We also echo that calibrating diverse document types across disciplines and different bibliographic databases remains a significant challenge (Haupka et al., 2024). For the next stage of this research, we will include additional metadata elements widely used in bibliometric analyses and investigate disciplinary differences in metadata quality. Further research will examine how metadata quality issues in OpenAlex could affect journal and institutional-level metrics and, thus, the results of institutional rankings like the open edition of the Leiden Ranking.

## Document-type discrepancies[\[MH1\]](#_msocom_1) 

## Publication year discrepancies[\[MH2\]](#_msocom_2) 

Pinning down a single published date value can possibly yield different dates that are valid. To gain a little more insight into the sources for both OpenAlex and WoS, we examined an API call[\[1\]](#_ftn1) for all 10 dates available in the Crossref metadata (posted, issued, updated-to, approved, indexed, accepted, published, published-print, published-online, deposited – three of these have to do with the published date). As an example, the DOI 10.1093/isle/isaa156 has three different unique dates that align with the OA and WoS values. The published and the published-online dates are 2020-11-30, and this agrees with the OpenAlex published date. But the published-print is 2021-12-13, and this agrees with the WoS values. So, both are correct. Yet, this work’s metadata was not deposited in Crossref until 2024-08-16. A single date, then, is tricky to pin down and does not completely capture the journey of a work through its scholarly lifecycle.

Overall, we found that published date values were preferred by OpenAlex, and WoS tends to use the issue date (476 works). 155 works were found that WoS used the published date, and OA used the issue date. The total of these (631 works) accounts for 67% of the works sampled, and the remainder have an omission, incorrect value, or the source was unclear with the date provided by either WoS or OpenAlex. Further work is needed to understand how local practices within publishers or other limitations may be affecting how publishers provide this information and make it available to scholarly databases.

## Language discrepancies[\[MH3\]](#_msocom_3) [\[MH4\]](#_msocom_4) [\[MH5\]](#_msocom_5) 

Reducing the language down to a single value for analysis is challenging and comes with many caveats. We observed that multi-language abstracts are not uncommon, though it is unknown if this is increasing.

Overall, there is an inconsistency in the single language value that is limited by both the OpenAlex and WoS metadata schema as many versions had multiple languages in the landing page, abstracts, and in the full text. This makes declaring a single language assignment difficult or even misrepresenting.

## Author discrepancies[\[MH6\]](#_msocom_6) [\[MH7\]](#_msocom_7) 

The author discrepancies subset was selected based on those in the dataset that had a difference between ‘oa_n_authors’ and ‘wos_n_authors’. As such, we find a few publishers that are over-represented as their local practices or limitations affect whether authors are entered into the metadata at all.

Overall, we see that OpenAlex was most correct on author counts on 53.7% of the subset of works, whereas WoS was correct on 33.2% of the subset. Regarding disciplines, the distribution is fairly even across disciplines for both OpenAlex and Wos, but BM and NSE are more affected by the ‘neither is correct’ condition, which is most likely affected by those authorships using consortia or other group identifiers. We also looked to see if there were similarities between OpenAlex and Crossref author counts and found that 50.3% of works contained the same authors, but alarmingly, we also found that 25.3% had author list errors, and 14.4% contained no authors at all. These omissions account for a large proportion of the works, and while both OpenAlex and WoS have other methods for extracting authors from landing pages and full text, publisher-supplied metadata should align with the published work.

The use of group terminology for authorships, such as consortia, research groups, study groups, teams, etc., seems to be common in the BM and NSE fields. There is great inconsistency in how these groups are accounted for, credit is given for authorship, how members are identified, or the language used to signify how authors work on behalf of or as part of a consortium. A few works in BM utilized the CRediT declaration and provided complete author lists, which clearly defined who was an author/contributor to the work. We also observed that group names may be listed as authors on the first page of the work or the landing page but were absent from the CRediT declaration. While the CRediT declaration made our job easier and established a clear authority by the authors on contributions, it is applied inconsistently. More work needs to be done to document these local practices to enable or encourage more consistency in author reporting and responsibilities.

We also observed consistently that OpenAlex excludes organization names as contributors, even if they exist in the Crossref metadata. Their process does not seem to affect singular names, such as those used in non-Western naming conventions, but it does seem to exclude consortia and other group identifiers in the author listing.

Conversely, WoS seems to be consistently overcounting author names, even if the Crossref metadata seems to align with the published version of the work. In contrast to the ‘lazy’ method of OpenAlex by excluding organization/group names, any pattern matching used by WoS seems to be greedy, accepting many more than there are listed in the work or author declarations. This seems to work, as most of the times that WoS was correct on the author count, the Crossref metadata was incorrect.

Translated works were an exception to the greedy approach by WoS, as their counts consistently excluded the number of translators on a work, mostly affecting works in the AH discipline. OpenAlex included both the author and translator on works, and this aligned with the Crossref metadata that also included translators as authors/contributors.

## Advantages and disadvantages in using each source[\[MH8\]](#_msocom_8) 

Haunschild and Bornmann (2024) offer useful insights from their use of OpenAlex data to produce overlay maps, one being that the main advantage of using OpenAlex data is that it can be used without restrictions (i.e., free availability). These are offset by concurrent limitations, such as in partial correctness or erroneous metadata, in their case, in OpenAlex assigned “concepts”[\[2\]](#_ftn2) raising questions about the reliability and validity of OpenAlex’s processes (Haunschild & Bornmann, 2024).

# Conclusion[\[MH9\]](#_msocom_9) [\[MH10\]](#_msocom_10) 

The idiosyncrasies of databases should be accounted for when researchers or research evaluators decide which to use for their data analysis tasks, as they affect the discoverability of works as well as metrics (Barbour et al., 2025). Differences among data sources regarding indexing coverage are, to a degree, largely understood and part of decision-making, Barbour et al. (2025) explain how the known selectivity of WoS and Scopus contrast with OpenAlex, which has a global and comprehensive aim. Metadata discrepancies, however, are more granular peculiarities and difficult to track.

The Paris Conference on Open Research Information and the Barcelona Declaration on Open Research Information are two initiatives that emphasize the need for and normalization of open research information. The Barcelona Declaration called for signatories to work with systems that support open research information (Barcelona Declaration, 2024). With the tide turning toward open data sources and researchers and institutions embracing OpenAlex and other open data sources and tools, more research will be needed on the quality and coverage of OpenAlex and the other data sources it depends on. This carries implications for OpenAlex, as they look to the research community for feedback on necessary improvements to their metadata, as well as for those conducting research and research evaluation using open sources, who must remain apprised of findings related to limitations of the data they use.

## Limitations

## Future work

Many open bibliographic data sources such as OpenAlex are downstream from metadata providers such as Crossref; as a crucial source for PubMed, OpenAIRE, OpenCitations, and, among other metadata digital infrastructure organizations, attention should be focused on the provenance of metadata, and their sustainability and reliability as a data stream for scholarly metadata (van Eck & Waltman, 2025). In noting that the quality of bibliographic metadata is quantifiable and measurable, Bruce and Hillman (2004) also note that enforcing standards must be done at the level of the community. Liu et al. (2021) make recommendations for remedying discrepancies: they recommend the use of official publication dates to avoid different publication years; this would also require the alignment of policies across databases. They also suggest databases work closely with journal publishers to mitigate document omission and metadata errors, double checking them frequently (Liu et al., 2021). Finally, they recommend the examination of data processing flows to identify potential causes when it comes to duplicate entries (Liu et al., 2021). To this list we also add…

# Conflicts of interest

The authors have no conflicts of interest to report.

# Author contributions

[Conceptualization](http://credit.niso.org/contributor-roles/conceptualization/) (PM), [Data curation](http://credit.niso.org/contributor-roles/data-curation/) (PM), [Formal analysis](http://credit.niso.org/contributor-roles/formal-analysis/) (PM, PR), Investigation (PM, MH, GK, RM, PR, RT, SW), [Project administration](http://credit.niso.org/contributor-roles/project-administration/) (PM), (PM), [Visualization](http://credit.niso.org/contributor-roles/visualization/) (RT, PR), [Writing – original draft](http://credit.niso.org/contributor-roles/writing-original-draft/) (PM, MH, PR), [Writing – review & editing](http://credit.niso.org/contributor-roles/writing-review-editing/) (PM, MH, GK, RM, PR, RT, SW)

# Data availability

# References

# Notes

[\[1\]](#_ftnref1) <https://api.crossref.org/works/?select=DOI,posted,issued,update-to,approved,indexed,accepted,published,published-print,published-online,deposited&filter=doi:10.1093/isle/isaa156> This API call selects all 10 date elements available in the Crossref REST API.

[\[2\]](#_ftnref2) OpenAlex Concepts have been deprecated in favor of Topics: <https://docs.openalex.org/api-entities/concepts>

 [\[MH1\]](#_msoanchor_1)[**https://link.springer.com/article/10.1007/s11192-017-2483-y#Sec6**](https://link.springer.com/article/10.1007/s11192-017-2483-y#Sec6)

**VERY useful for doc types.**

 [\[MH2\]](#_msoanchor_2)<https://arxiv.org/abs/1505.00796>

 [\[MH3\]](#_msoanchor_3)From Shi et al. (2025): The lack of standardized and widely adopted Romanization schemes for many languages

itself results in errors and inconsistencies: localized standards may be developed and used

in isolation; when multiple schemes exist like this, guidance may be referenced and applied

inconsistently (Park, 2007); or Romanized forms may be decided on independent of any guidance.

Moreover, the choice to record Romanizations only may preclude access to resources

by users unfamiliar with such schemes or who would transcribe or transliterate differently

(Rigby, 2015). This raises further ethical questions about who metadata caters to when rendered

only in translation, transcription, or transliteration.d

·          [\[MH4\]](#_msoanchor_4)Vera-Baceta, M.-A., Thelwall, M., & Kousha, K. (2019). Web of Science and Scopus language coverage. *Scientometrics*, *121*(3), 1803–1813. <https://doi.org/10.1007/s11192-019-03264-z>\
\
English is by far the dominant language in both Scopus (at \>92%) and WoS (\>95%). Generally, WoS has less coverage of most (non-English) languages than Scopus, and in particular, has less than 1/10 as many works in Chinese (which comes in second, at 2.76%, in Scopus). WoS does have higher coverage of some languages, including Spanish. These numbers differ somewhat from studies before 2015, as WoS has since added the Emerging Sources Citation Index to its core collection, expanding coverage of non-English works.

·         Language coverage varies widely across subject areas.

·          [\[MH5\]](#_msoanchor_5)Cespedes:\
\
Find that even accounting for errors, works in OpenAlex have far greater linguistic diversity than Web of Science. WoS is more than 95% English, whereas the OpenAlex sample is identified as such by the metadata 75% of the time.

·         Manual examination of the works themselves (vs. metadata) brings this number closer to 68%. English is thus overestimated, as are other Western European languages, including French and German. Chinese and Russian are underreported.

·         The language field is populated for 98% of the documents, and is true to the underlying document for more than 85%.

·         Lack or presence of DOIs does not seem to impact accuracy of language.

·         Point out that the OpenAlex language field is algorithmically derived from the metadata (title & abstract), and that the *langdetect* algorithm is limited to 55 languages.

 [\[MH6\]](#_msoanchor_6)Strotmann, A., & Zhao, D. (2012). Author name disambiguation: What difference does it make in author‐based citation analysis? Journal of the American Society for Information Science and Technology, 63(9), 1820–1833. https://doi.org/10.1002/asi.22695

·          [\[MH7\]](#_msoanchor_7)Without complex author disambiguation, even in a single field (stem cell research) over a short period (2004-2009), more than a third of “top authors” wind up being conflated groups of hundreds of individuals sharing a last name + first initial. Chinese and Korean names are particularly affected by this. The differences between author co-citation analyses with and without advanced disambiguation is significant when such authors are present, and the effect is generally a complex one, rather than simply the conflation of pairs of authors.

·         Mentions that Web of Science could be a problem, given the possibility of omitting the last author (which is generally easier to disambiguate, given the lower number of name collisions, and lower prevalence of Chinese/Korean names in this position).

 [\[MH8\]](#_msoanchor_8)<https://osf.io/preprints/metaarxiv/smxe5_v2>

 [\[MH9\]](#_msoanchor_9)Oncepossibledatabaseerrorsareidentified,theycanbenotifiedtothedatabasestaffthroughdedicatedsupport/feedback mechanisms. We have noticed that Scopus and WoS are both very responsive to these feedbacks (Meester et al., 2016). As regards database administrators, at the risk of being repetitive, we renew our exhortation to improve in terms of data cleaning. We remark that all the database errors analyzed and classified in this research were preventable: in fact, all the citations omitted by one database are, by definition, correctly indexed by the other one. Weareawarethatthecitation-matchingalgorithmsusedbydatabaseswillneverbeinfallible,astheystruggletofindthe optimalbalancebetween(i)theriskoffailingtoidentifyauthenticcitations(falsenegatives)and(ii)thatofassigningphantom citations (false positives). Nevertheless, we believe that databases could introduce additional (automated) controls on the results of the citation mapping process (e.g., not to reinvent the wheel, the automated algorithm presented in (Franceschini et al., 2013)). This would be much more effective than waiting for the feedbacks from users, with an important benefit in terms of image. In the interest of the entire scientific community and their own one, we hope that Scopus and WoS will invest in such improvements.

# 
